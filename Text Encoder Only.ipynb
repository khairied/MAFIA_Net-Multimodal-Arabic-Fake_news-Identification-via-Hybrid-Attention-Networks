{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11118864,"sourceType":"datasetVersion","datasetId":6933281}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# =========================\n# INSTALLS (if needed)\n# =========================\n# !pip install -U sentence-transformers scikit-learn torch transformers tqdm\n\n# =========================\n# IMPORTS\n# =========================\nimport numpy as np\nimport pandas as pd\nimport torch\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW \nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom tqdm import tqdm\nimport os, random, torch, numpy as np\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    ##torch.backends.cudnn.deterministic = False\n    ##torch.backends.cudnn.benchmark = True\n    torch.use_deterministic_algorithms(True, warn_only=False)\n    print(f\"[INFO] All seeds set to {seed}\")\n\nseed_everything(42)\n\n\ndef seed_worker(worker_id):\n    worker_seed = torch.initial_seed() % 2**32\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)\n\ng = torch.Generator()\ng.manual_seed(42)\n\ndef free_gpu():\n    torch.cuda.empty_cache()\n    torch.cuda.ipc_collect()\n##set_seed(42)\n# =========================\n# CONFIG\n# =========================\n\n\n\nMODELS_NAMES= [\"aubmindlab/bert-base-arabert\",\"UBC-NLP/MARBERTv2\",\n              \"aubmindlab/araelectra-base-generator\",\n               \"3ebdola/Dialectal-Arabic-XLM-R-Base\", \n               \"aubmindlab/araelectra-base-discriminator\",\n               \"bashar-talafha/multi-dialect-bert-base-arabic\",\n              \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n              \"sentence-transformers/distilbert-multilingual-nli-stsb-quora-ranking\",\n              \"sentence-transformers/distiluse-base-multilingual-cased\",\n               \"sentence-transformers/clip-ViT-B-32-multilingual-v1\",\n               \"sentence-transformers/distiluse-base-multilingual-cased-v1\",\n               \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n              ]\n\n\nNUM_EPOCHS = 3\nBATCH_SIZE = 8\nLR = 2e-5\nN_SPLITS = 5\nMAX_LEN = 64\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# =========================\n# LOAD YOUR DATA\n# =========================\n# Example:\ndf = pd.read_csv(\"/kaggle/input/ekafnewsforkhawla/shuffled_cleaned_text_file for khawla.txt\",sep='\\t',encoding=\"utf-8\")\ndf = df[[\"preprocess1\", \"label\", \"dataset\"]]\n# df must contain: text | label\n\n# Dummy placeholder (REMOVE THIS)\n# df = pd.DataFrame({\n#     \"text\": [\"example text one\", \"example text two\"],\n#     \"label\": [0, 1]\n# })\n\ntexts = df[\"preprocess1\"].astype(str).values\nlabels = df[\"label\"].astype(int).values\nnum_labels = len(np.unique(labels))\n\n# =========================\n# DATASET CLASS\n# =========================\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        enc = self.tokenizer(\n            self.texts[idx],\n            truncation=True,\n            padding=\"max_length\",\n            max_length=MAX_LEN,\n            return_tensors=\"pt\"\n        )\n        return {\n            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n        }\nfor MODEL_NAME in MODELS_NAMES:\n    # =========================\n    # TOKENIZER\n    # =========================\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\n    # =========================\n    # CROSS VALIDATION\n    # =========================\n    strat = df['label']\n\n    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=False, random_state=None)\n\n\n    all_preds = []\n    all_labels = []\n\n    for fold, (train_idx, val_idx) in enumerate(skf.split(texts, strat), 1):\n        free_gpu()\n        print(f\"\\n{'='*30}\")\n        print(f\"FOLD {fold}\")\n        print(f\"{'='*30}\")\n        \n        # Datasets\n        train_ds = TextDataset(texts[train_idx], labels[train_idx], tokenizer)\n        val_ds   = TextDataset(texts[val_idx], labels[val_idx], tokenizer)\n\n        train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=False)\n        val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n\n        # Model\n        model = AutoModelForSequenceClassification.from_pretrained(\n            MODEL_NAME,\n            num_labels=num_labels\n        ).to(DEVICE)\n\n        optimizer = AdamW(model.parameters(), lr=LR)\n\n        # =========================\n        # TRAINING\n        # =========================\n        model.train()\n        for epoch in range(NUM_EPOCHS):\n            loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n            for batch in loop:\n                optimizer.zero_grad()\n\n                outputs = model(\n                    input_ids=batch[\"input_ids\"].to(DEVICE),\n                    attention_mask=batch[\"attention_mask\"].to(DEVICE),\n                    labels=batch[\"labels\"].to(DEVICE)\n                )\n\n                loss = outputs.loss\n                loss.backward()\n                optimizer.step()\n\n                loop.set_postfix(loss=loss.item())\n\n        # =========================\n        # VALIDATION\n        # =========================\n        model.eval()\n        fold_preds = []\n        fold_labels = []\n\n        with torch.no_grad():\n            for batch in val_loader:\n                outputs = model(\n                    input_ids=batch[\"input_ids\"].to(DEVICE),\n                    attention_mask=batch[\"attention_mask\"].to(DEVICE)\n                )\n                preds = torch.argmax(outputs.logits, dim=1)\n\n                fold_preds.extend(preds.cpu().numpy())\n                fold_labels.extend(batch[\"labels\"].numpy())\n\n        # Store global results\n        all_preds.extend(fold_preds)\n        all_labels.extend(fold_labels)\n\n        # =========================\n        # REPORTS PER FOLD\n        # =========================\n        print(\"\\nClassification Report (Fold {})\".format(fold))\n        print(classification_report(fold_labels, fold_preds, digits=4))\n\n        print(\"Confusion Matrix (Fold {})\".format(fold))\n        print(confusion_matrix(fold_labels, fold_preds))\n\n    # =========================\n    # FINAL GLOBAL RESULTS\n    # =========================\n    print(f\"\\n{'✅'*50}\")\n    print(\"OVERALL RESULTS (ALL FOLDS)\")\n    print(MODEL_NAME)\n    print(f\"{'✅'*50}\")\n\n    print(\"\\nFinal Classification Report\")\n    print(classification_report(all_labels, all_preds, digits=4))\n\n    print(\"Final Confusion Matrix\")\n    print(confusion_matrix(all_labels, all_preds))\n    print(f\"\\n{'✅'*50}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-01T18:48:36.505815Z","iopub.execute_input":"2026-02-01T18:48:36.506448Z","iopub.status.idle":"2026-02-01T20:47:20.947196Z","shell.execute_reply.started":"2026-02-01T18:48:36.506418Z","shell.execute_reply":"2026-02-01T20:47:20.946412Z"}},"outputs":[{"name":"stdout","text":"[INFO] All seeds set to 42\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/637 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a440ce2a2b104d22b8a147d8db6a2451"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/578 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"968c616db08340c2be836bbbdebc2c8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c39b076245b2455092a3e3ef10b72a69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3430d7fb1c1d4c03b0ac0fa4ea38ee5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5e8547a473949b88d82db8072bc9095"}},"metadata":{}},{"name":"stdout","text":"\n==============================\nFOLD 1\n==============================\n","output_type":"stream"},{"name":"stderr","text":"2026-02-01 18:48:52.655615: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1769971732.830493      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1769971732.882622      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1769971733.316529      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769971733.316567      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769971733.316570      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769971733.316572      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/543M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9ca1ca49bf644e0886135abc0157876"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabert and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:41<00:00, 12.39it/s, loss=0.567] \nEpoch 2: 100%|██████████| 514/514 [00:40<00:00, 12.63it/s, loss=0.107]  \nEpoch 3: 100%|██████████| 514/514 [00:40<00:00, 12.62it/s, loss=0.185]  \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 1)\n              precision    recall  f1-score   support\n\n           0     0.7560    0.8088    0.7815       272\n           1     0.9294    0.9061    0.9176       756\n\n    accuracy                         0.8804      1028\n   macro avg     0.8427    0.8575    0.8496      1028\nweighted avg     0.8836    0.8804    0.8816      1028\n\nConfusion Matrix (Fold 1)\n[[220  52]\n [ 71 685]]\n\n==============================\nFOLD 2\n==============================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabert and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:40<00:00, 12.59it/s, loss=0.344] \nEpoch 2: 100%|██████████| 514/514 [00:40<00:00, 12.59it/s, loss=0.0528] \nEpoch 3: 100%|██████████| 514/514 [00:40<00:00, 12.59it/s, loss=0.00665] \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 2)\n              precision    recall  f1-score   support\n\n           0     0.6882    0.8603    0.7647       272\n           1     0.9448    0.8598    0.9003       756\n\n    accuracy                         0.8599      1028\n   macro avg     0.8165    0.8600    0.8325      1028\nweighted avg     0.8769    0.8599    0.8644      1028\n\nConfusion Matrix (Fold 2)\n[[234  38]\n [106 650]]\n\n==============================\nFOLD 3\n==============================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabert and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:40<00:00, 12.58it/s, loss=0.226] \nEpoch 2: 100%|██████████| 514/514 [00:40<00:00, 12.58it/s, loss=0.0267] \nEpoch 3: 100%|██████████| 514/514 [00:40<00:00, 12.59it/s, loss=0.0585] \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 3)\n              precision    recall  f1-score   support\n\n           0     0.8197    0.7326    0.7737       273\n           1     0.9069    0.9417    0.9240       755\n\n    accuracy                         0.8862      1028\n   macro avg     0.8633    0.8372    0.8488      1028\nweighted avg     0.8837    0.8862    0.8841      1028\n\nConfusion Matrix (Fold 3)\n[[200  73]\n [ 44 711]]\n\n==============================\nFOLD 4\n==============================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabert and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:40<00:00, 12.58it/s, loss=0.64]  \nEpoch 2: 100%|██████████| 514/514 [00:40<00:00, 12.59it/s, loss=0.0584] \nEpoch 3: 100%|██████████| 514/514 [00:40<00:00, 12.58it/s, loss=0.095]   \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 4)\n              precision    recall  f1-score   support\n\n           0     0.7857    0.6875    0.7333       272\n           1     0.8923    0.9325    0.9119       755\n\n    accuracy                         0.8676      1027\n   macro avg     0.8390    0.8100    0.8226      1027\nweighted avg     0.8640    0.8676    0.8646      1027\n\nConfusion Matrix (Fold 4)\n[[187  85]\n [ 51 704]]\n\n==============================\nFOLD 5\n==============================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabert and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:40<00:00, 12.58it/s, loss=0.155] \nEpoch 2: 100%|██████████| 514/514 [00:40<00:00, 12.61it/s, loss=0.0119] \nEpoch 3: 100%|██████████| 514/514 [00:40<00:00, 12.61it/s, loss=0.00975] \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 5)\n              precision    recall  f1-score   support\n\n           0     0.7574    0.7574    0.7574       272\n           1     0.9126    0.9126    0.9126       755\n\n    accuracy                         0.8715      1027\n   macro avg     0.8350    0.8350    0.8350      1027\nweighted avg     0.8715    0.8715    0.8715      1027\n\nConfusion Matrix (Fold 5)\n[[206  66]\n [ 66 689]]\n\n✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\nOVERALL RESULTS (ALL FOLDS)\naubmindlab/bert-base-arabert\n✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\n\nFinal Classification Report\n              precision    recall  f1-score   support\n\n           0     0.7560    0.7693    0.7626      1361\n           1     0.9163    0.9105    0.9134      3777\n\n    accuracy                         0.8731      5138\n   macro avg     0.8361    0.8399    0.8380      5138\nweighted avg     0.8739    0.8731    0.8735      5138\n\nFinal Confusion Matrix\n[[1047  314]\n [ 338 3439]]\n\n✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/439 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"329daf3efda14ee7841acbc8ac280924"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f76aa41619e444f89a2cda97f475cbf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fef1f5a3b0a4669904514a669b01acc"}},"metadata":{}},{"name":"stdout","text":"\n==============================\nFOLD 1\n==============================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/757 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c223c626df0f47ecac60f63b0ca9ab95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/654M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55e9cc756b314edda7bf064883035a82"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at UBC-NLP/MARBERTv2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1:   1%|          | 5/514 [00:00<00:44, 11.44it/s, loss=0.416]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/654M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a16ffa486b94705b6f858420a2ff019"}},"metadata":{}},{"name":"stderr","text":"Epoch 1: 100%|██████████| 514/514 [00:44<00:00, 11.58it/s, loss=0.314] \nEpoch 2: 100%|██████████| 514/514 [00:43<00:00, 11.84it/s, loss=0.0671] \nEpoch 3: 100%|██████████| 514/514 [00:43<00:00, 11.86it/s, loss=0.00374]\n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 1)\n              precision    recall  f1-score   support\n\n           0     0.8715    0.7978    0.8330       272\n           1     0.9294    0.9577    0.9433       756\n\n    accuracy                         0.9154      1028\n   macro avg     0.9004    0.8777    0.8882      1028\nweighted avg     0.9141    0.9154    0.9141      1028\n\nConfusion Matrix (Fold 1)\n[[217  55]\n [ 32 724]]\n\n==============================\nFOLD 2\n==============================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at UBC-NLP/MARBERTv2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:43<00:00, 11.77it/s, loss=0.259] \nEpoch 2: 100%|██████████| 514/514 [00:43<00:00, 11.83it/s, loss=0.0207] \nEpoch 3: 100%|██████████| 514/514 [00:43<00:00, 11.84it/s, loss=0.00496]\n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 2)\n              precision    recall  f1-score   support\n\n           0     0.8225    0.8346    0.8285       272\n           1     0.9402    0.9352    0.9377       756\n\n    accuracy                         0.9086      1028\n   macro avg     0.8813    0.8849    0.8831      1028\nweighted avg     0.9090    0.9086    0.9088      1028\n\nConfusion Matrix (Fold 2)\n[[227  45]\n [ 49 707]]\n\n==============================\nFOLD 3\n==============================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at UBC-NLP/MARBERTv2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:43<00:00, 11.83it/s, loss=0.239] \nEpoch 2: 100%|██████████| 514/514 [00:43<00:00, 11.85it/s, loss=0.0344] \nEpoch 3: 100%|██████████| 514/514 [00:43<00:00, 11.86it/s, loss=0.00929]\n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 3)\n              precision    recall  f1-score   support\n\n           0     0.8615    0.7289    0.7897       273\n           1     0.9072    0.9576    0.9317       755\n\n    accuracy                         0.8969      1028\n   macro avg     0.8843    0.8433    0.8607      1028\nweighted avg     0.8950    0.8969    0.8940      1028\n\nConfusion Matrix (Fold 3)\n[[199  74]\n [ 32 723]]\n\n==============================\nFOLD 4\n==============================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at UBC-NLP/MARBERTv2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:43<00:00, 11.85it/s, loss=0.216] \nEpoch 2: 100%|██████████| 514/514 [00:43<00:00, 11.84it/s, loss=0.0304] \nEpoch 3: 100%|██████████| 514/514 [00:43<00:00, 11.86it/s, loss=0.118]  \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 4)\n              precision    recall  f1-score   support\n\n           0     0.7196    0.8493    0.7791       272\n           1     0.9419    0.8808    0.9103       755\n\n    accuracy                         0.8724      1027\n   macro avg     0.8308    0.8650    0.8447      1027\nweighted avg     0.8831    0.8724    0.8756      1027\n\nConfusion Matrix (Fold 4)\n[[231  41]\n [ 90 665]]\n\n==============================\nFOLD 5\n==============================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at UBC-NLP/MARBERTv2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:43<00:00, 11.86it/s, loss=0.2]   \nEpoch 2: 100%|██████████| 514/514 [00:43<00:00, 11.87it/s, loss=0.245]  \nEpoch 3: 100%|██████████| 514/514 [00:43<00:00, 11.87it/s, loss=0.017]  \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 5)\n              precision    recall  f1-score   support\n\n           0     0.8879    0.7279    0.8000       272\n           1     0.9080    0.9669    0.9365       755\n\n    accuracy                         0.9036      1027\n   macro avg     0.8979    0.8474    0.8682      1027\nweighted avg     0.9026    0.9036    0.9003      1027\n\nConfusion Matrix (Fold 5)\n[[198  74]\n [ 25 730]]\n\n✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\nOVERALL RESULTS (ALL FOLDS)\nUBC-NLP/MARBERTv2\n✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\n\nFinal Classification Report\n              precision    recall  f1-score   support\n\n           0     0.8246    0.7877    0.8057      1361\n           1     0.9247    0.9396    0.9321      3777\n\n    accuracy                         0.8994      5138\n   macro avg     0.8747    0.8636    0.8689      5138\nweighted avg     0.8982    0.8994    0.8986      5138\n\nFinal Confusion Matrix\n[[1072  289]\n [ 228 3549]]\n\n✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/393 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f9f36f09ffb422d92d5f806476ca250"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/499 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77975151319a4d45b1f8473f020ddd47"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7195997e32814abe902d7e8300db16ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e0f1a6714d94a3aa3722e776fb0cd36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10e17abffe444869a2d8942992f1e9fc"}},"metadata":{}},{"name":"stdout","text":"\n==============================\nFOLD 1\n==============================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/238M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09973e00eae544f29b236c249d6418ec"}},"metadata":{}},{"name":"stderr","text":"Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at aubmindlab/araelectra-base-generator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:32<00:00, 15.98it/s, loss=0.777]\nEpoch 2: 100%|██████████| 514/514 [00:32<00:00, 16.05it/s, loss=0.661] \nEpoch 3: 100%|██████████| 514/514 [00:32<00:00, 16.06it/s, loss=0.72]  \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 1)\n              precision    recall  f1-score   support\n\n           0     0.7834    0.7978    0.7905       272\n           1     0.9268    0.9206    0.9237       756\n\n    accuracy                         0.8881      1028\n   macro avg     0.8551    0.8592    0.8571      1028\nweighted avg     0.8888    0.8881    0.8885      1028\n\nConfusion Matrix (Fold 1)\n[[217  55]\n [ 60 696]]\n\n==============================\nFOLD 2\n==============================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at aubmindlab/araelectra-base-generator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:32<00:00, 15.97it/s, loss=0.693]\nEpoch 2: 100%|██████████| 514/514 [00:32<00:00, 16.01it/s, loss=0.622] \nEpoch 3: 100%|██████████| 514/514 [00:31<00:00, 16.08it/s, loss=0.631] \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 2)\n              precision    recall  f1-score   support\n\n           0     0.8051    0.8051    0.8051       272\n           1     0.9299    0.9299    0.9299       756\n\n    accuracy                         0.8969      1028\n   macro avg     0.8675    0.8675    0.8675      1028\nweighted avg     0.8969    0.8969    0.8969      1028\n\nConfusion Matrix (Fold 2)\n[[219  53]\n [ 53 703]]\n\n==============================\nFOLD 3\n==============================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at aubmindlab/araelectra-base-generator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:32<00:00, 15.98it/s, loss=1.04] \nEpoch 2: 100%|██████████| 514/514 [00:32<00:00, 16.05it/s, loss=1.06]  \nEpoch 3: 100%|██████████| 514/514 [00:32<00:00, 16.06it/s, loss=0.653] \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 3)\n              precision    recall  f1-score   support\n\n           0     0.6855    0.7985    0.7377       273\n           1     0.9225    0.8675    0.8942       755\n\n    accuracy                         0.8492      1028\n   macro avg     0.8040    0.8330    0.8160      1028\nweighted avg     0.8596    0.8492    0.8526      1028\n\nConfusion Matrix (Fold 3)\n[[218  55]\n [100 655]]\n\n==============================\nFOLD 4\n==============================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at aubmindlab/araelectra-base-generator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:32<00:00, 16.04it/s, loss=0.638]\nEpoch 2: 100%|██████████| 514/514 [00:31<00:00, 16.07it/s, loss=0.403] \nEpoch 3: 100%|██████████| 514/514 [00:32<00:00, 16.02it/s, loss=0.142] \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 4)\n              precision    recall  f1-score   support\n\n           0     0.7647    0.7169    0.7400       272\n           1     0.9003    0.9205    0.9103       755\n\n    accuracy                         0.8666      1027\n   macro avg     0.8325    0.8187    0.8252      1027\nweighted avg     0.8644    0.8666    0.8652      1027\n\nConfusion Matrix (Fold 4)\n[[195  77]\n [ 60 695]]\n\n==============================\nFOLD 5\n==============================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at aubmindlab/araelectra-base-generator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:32<00:00, 16.01it/s, loss=0.279]\nEpoch 2: 100%|██████████| 514/514 [00:32<00:00, 16.01it/s, loss=0.0781]\nEpoch 3: 100%|██████████| 514/514 [00:32<00:00, 16.00it/s, loss=0.0363]\n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 5)\n              precision    recall  f1-score   support\n\n           0     0.7732    0.8272    0.7993       272\n           1     0.9361    0.9126    0.9242       755\n\n    accuracy                         0.8900      1027\n   macro avg     0.8547    0.8699    0.8618      1027\nweighted avg     0.8930    0.8900    0.8911      1027\n\nConfusion Matrix (Fold 5)\n[[225  47]\n [ 66 689]]\n\n✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\nOVERALL RESULTS (ALL FOLDS)\naubmindlab/araelectra-base-generator\n✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\n\nFinal Classification Report\n              precision    recall  f1-score   support\n\n           0     0.7601    0.7891    0.7743      1361\n           1     0.9230    0.9102    0.9166      3777\n\n    accuracy                         0.8782      5138\n   macro avg     0.8415    0.8497    0.8454      5138\nweighted avg     0.8798    0.8782    0.8789      5138\n\nFinal Confusion Matrix\n[[1074  287]\n [ 339 3438]]\n\n✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/536 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3931ea89395462380f20dc5c8af2a28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78a29b1a64a24198af8d57bb8d7a55c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c88bfe5c3a84fac9a0d76e3103b9002"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4774736e90a14723b5be37810b16eb61"}},"metadata":{}},{"name":"stdout","text":"\n==============================\nFOLD 1\n==============================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/753 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"565c5b90cec4427bb0ef82c84f6b9bdb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.11G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"587f7457e385409fb825616a7ad892e2"}},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at 3ebdola/Dialectal-Arabic-XLM-R-Base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1:   1%|          | 3/514 [00:00<01:00,  8.39it/s, loss=0.48] ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"931ed6ea5acc450192164cb526988301"}},"metadata":{}},{"name":"stderr","text":"Epoch 1: 100%|██████████| 514/514 [00:56<00:00,  9.13it/s, loss=0.411] \nEpoch 2: 100%|██████████| 514/514 [00:54<00:00,  9.39it/s, loss=0.0734] \nEpoch 3: 100%|██████████| 514/514 [00:54<00:00,  9.36it/s, loss=0.0553]  \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 1)\n              precision    recall  f1-score   support\n\n           0     0.7047    0.8860    0.7850       272\n           1     0.9548    0.8664    0.9085       756\n\n    accuracy                         0.8716      1028\n   macro avg     0.8297    0.8762    0.8467      1028\nweighted avg     0.8886    0.8716    0.8758      1028\n\nConfusion Matrix (Fold 1)\n[[241  31]\n [101 655]]\n\n==============================\nFOLD 2\n==============================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at 3ebdola/Dialectal-Arabic-XLM-R-Base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:54<00:00,  9.40it/s, loss=0.585] \nEpoch 2: 100%|██████████| 514/514 [00:54<00:00,  9.39it/s, loss=0.23]   \nEpoch 3: 100%|██████████| 514/514 [00:54<00:00,  9.40it/s, loss=0.018]   \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 2)\n              precision    recall  f1-score   support\n\n           0     0.7251    0.8824    0.7960       272\n           1     0.9541    0.8796    0.9153       756\n\n    accuracy                         0.8804      1028\n   macro avg     0.8396    0.8810    0.8557      1028\nweighted avg     0.8935    0.8804    0.8838      1028\n\nConfusion Matrix (Fold 2)\n[[240  32]\n [ 91 665]]\n\n==============================\nFOLD 3\n==============================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at 3ebdola/Dialectal-Arabic-XLM-R-Base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:54<00:00,  9.36it/s, loss=0.592] \nEpoch 2: 100%|██████████| 514/514 [00:54<00:00,  9.37it/s, loss=0.292]  \nEpoch 3: 100%|██████████| 514/514 [00:54<00:00,  9.39it/s, loss=0.0249] \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 3)\n              precision    recall  f1-score   support\n\n           0     0.8017    0.7106    0.7534       273\n           1     0.8995    0.9364    0.9176       755\n\n    accuracy                         0.8765      1028\n   macro avg     0.8506    0.8235    0.8355      1028\nweighted avg     0.8735    0.8765    0.8740      1028\n\nConfusion Matrix (Fold 3)\n[[194  79]\n [ 48 707]]\n\n==============================\nFOLD 4\n==============================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at 3ebdola/Dialectal-Arabic-XLM-R-Base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:54<00:00,  9.37it/s, loss=0.505] \nEpoch 2: 100%|██████████| 514/514 [00:54<00:00,  9.39it/s, loss=0.134]  \nEpoch 3: 100%|██████████| 514/514 [00:54<00:00,  9.39it/s, loss=0.0483] \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 4)\n              precision    recall  f1-score   support\n\n           0     0.6772    0.8640    0.7593       272\n           1     0.9456    0.8517    0.8962       755\n\n    accuracy                         0.8549      1027\n   macro avg     0.8114    0.8578    0.8277      1027\nweighted avg     0.8745    0.8549    0.8599      1027\n\nConfusion Matrix (Fold 4)\n[[235  37]\n [112 643]]\n\n==============================\nFOLD 5\n==============================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at 3ebdola/Dialectal-Arabic-XLM-R-Base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:54<00:00,  9.39it/s, loss=0.214] \nEpoch 2: 100%|██████████| 514/514 [00:54<00:00,  9.41it/s, loss=0.0277]\nEpoch 3: 100%|██████████| 514/514 [00:54<00:00,  9.39it/s, loss=0.00395]\n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 5)\n              precision    recall  f1-score   support\n\n           0     0.6771    0.8787    0.7648       272\n           1     0.9510    0.8490    0.8971       755\n\n    accuracy                         0.8569      1027\n   macro avg     0.8140    0.8638    0.8310      1027\nweighted avg     0.8785    0.8569    0.8621      1027\n\nConfusion Matrix (Fold 5)\n[[239  33]\n [114 641]]\n\n✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\nOVERALL RESULTS (ALL FOLDS)\n3ebdola/Dialectal-Arabic-XLM-R-Base\n✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\n\nFinal Classification Report\n              precision    recall  f1-score   support\n\n           0     0.7115    0.8442    0.7722      1361\n           1     0.9398    0.8766    0.9071      3777\n\n    accuracy                         0.8680      5138\n   macro avg     0.8256    0.8604    0.8397      5138\nweighted avg     0.8793    0.8680    0.8714      5138\n\nFinal Confusion Matrix\n[[1149  212]\n [ 466 3311]]\n\n✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/392 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb4b7f48cfb849fabca2a56914add535"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/503 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0196edf3ceb5464fba5373c0c9f03f90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69773204bd214a3a9209511b599b148d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5eb86f54832446b80a7315da8dc73e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae12ea59d3524c8d8223bd64b9923d0e"}},"metadata":{}},{"name":"stdout","text":"\n==============================\nFOLD 1\n==============================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/541M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2221dc99929e4f07a20e7c49309ac5e1"}},"metadata":{}},{"name":"stderr","text":"Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at aubmindlab/araelectra-base-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:43<00:00, 11.79it/s, loss=0.414] \nEpoch 2: 100%|██████████| 514/514 [00:43<00:00, 11.92it/s, loss=0.0709] \nEpoch 3: 100%|██████████| 514/514 [00:43<00:00, 11.94it/s, loss=0.00878]\n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 1)\n              precision    recall  f1-score   support\n\n           0     0.7447    0.9007    0.8153       272\n           1     0.9614    0.8889    0.9237       756\n\n    accuracy                         0.8920      1028\n   macro avg     0.8530    0.8948    0.8695      1028\nweighted avg     0.9040    0.8920    0.8950      1028\n\nConfusion Matrix (Fold 1)\n[[245  27]\n [ 84 672]]\n\n==============================\nFOLD 2\n==============================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at aubmindlab/araelectra-base-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:43<00:00, 11.91it/s, loss=0.293] \nEpoch 2: 100%|██████████| 514/514 [00:43<00:00, 11.93it/s, loss=0.104]  \nEpoch 3: 100%|██████████| 514/514 [00:43<00:00, 11.91it/s, loss=0.0587] \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 2)\n              precision    recall  f1-score   support\n\n           0     0.8248    0.7096    0.7628       272\n           1     0.9005    0.9458    0.9226       756\n\n    accuracy                         0.8833      1028\n   macro avg     0.8626    0.8277    0.8427      1028\nweighted avg     0.8805    0.8833    0.8803      1028\n\nConfusion Matrix (Fold 2)\n[[193  79]\n [ 41 715]]\n\n==============================\nFOLD 3\n==============================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at aubmindlab/araelectra-base-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:43<00:00, 11.94it/s, loss=0.686] \nEpoch 2: 100%|██████████| 514/514 [00:43<00:00, 11.94it/s, loss=0.0827] \nEpoch 3: 100%|██████████| 514/514 [00:42<00:00, 11.95it/s, loss=0.0106] \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 3)\n              precision    recall  f1-score   support\n\n           0     0.8578    0.7070    0.7751       273\n           1     0.9004    0.9576    0.9281       755\n\n    accuracy                         0.8911      1028\n   macro avg     0.8791    0.8323    0.8516      1028\nweighted avg     0.8891    0.8911    0.8875      1028\n\nConfusion Matrix (Fold 3)\n[[193  80]\n [ 32 723]]\n\n==============================\nFOLD 4\n==============================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at aubmindlab/araelectra-base-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:43<00:00, 11.90it/s, loss=0.52]  \nEpoch 2: 100%|██████████| 514/514 [00:43<00:00, 11.89it/s, loss=0.0458] \nEpoch 3: 100%|██████████| 514/514 [00:43<00:00, 11.92it/s, loss=0.0127] \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 4)\n              precision    recall  f1-score   support\n\n           0     0.5664    0.9559    0.7114       272\n           1     0.9789    0.7364    0.8405       755\n\n    accuracy                         0.7945      1027\n   macro avg     0.7727    0.8462    0.7759      1027\nweighted avg     0.8696    0.7945    0.8063      1027\n\nConfusion Matrix (Fold 4)\n[[260  12]\n [199 556]]\n\n==============================\nFOLD 5\n==============================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at aubmindlab/araelectra-base-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:43<00:00, 11.92it/s, loss=0.126] \nEpoch 2: 100%|██████████| 514/514 [00:43<00:00, 11.91it/s, loss=0.0307] \nEpoch 3: 100%|██████████| 514/514 [00:43<00:00, 11.92it/s, loss=0.0118] \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 5)\n              precision    recall  f1-score   support\n\n           0     0.7354    0.8787    0.8007       272\n           1     0.9530    0.8861    0.9183       755\n\n    accuracy                         0.8841      1027\n   macro avg     0.8442    0.8824    0.8595      1027\nweighted avg     0.8954    0.8841    0.8872      1027\n\nConfusion Matrix (Fold 5)\n[[239  33]\n [ 86 669]]\n\n✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\nOVERALL RESULTS (ALL FOLDS)\naubmindlab/araelectra-base-discriminator\n✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\n\nFinal Classification Report\n              precision    recall  f1-score   support\n\n           0     0.7188    0.8303    0.7705      1361\n           1     0.9352    0.8830    0.9083      3777\n\n    accuracy                         0.8690      5138\n   macro avg     0.8270    0.8566    0.8394      5138\nweighted avg     0.8779    0.8690    0.8718      5138\n\nFinal Confusion Matrix\n[[1130  231]\n [ 442 3335]]\n\n✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/456 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"893d8ec711034942a47a17683019d106"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"545eca87c5fa4eac96b0b5b180fbc506"}},"metadata":{}},{"name":"stdout","text":"\n==============================\nFOLD 1\n==============================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/445M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3f831e10f6044e289e583858dffd03c"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bashar-talafha/multi-dialect-bert-base-arabic and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1:   3%|▎         | 16/514 [00:01<00:37, 13.17it/s, loss=0.995] ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/445M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2982e43c98845a787c89f9c767fb9d8"}},"metadata":{}},{"name":"stderr","text":"Epoch 1: 100%|██████████| 514/514 [00:39<00:00, 13.02it/s, loss=0.522] \nEpoch 2: 100%|██████████| 514/514 [00:38<00:00, 13.26it/s, loss=0.055]  \nEpoch 3: 100%|██████████| 514/514 [00:38<00:00, 13.26it/s, loss=0.0022]  \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 1)\n              precision    recall  f1-score   support\n\n           0     0.7417    0.9081    0.8165       272\n           1     0.9640    0.8862    0.9235       756\n\n    accuracy                         0.8920      1028\n   macro avg     0.8529    0.8972    0.8700      1028\nweighted avg     0.9052    0.8920    0.8952      1028\n\nConfusion Matrix (Fold 1)\n[[247  25]\n [ 86 670]]\n\n==============================\nFOLD 2\n==============================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bashar-talafha/multi-dialect-bert-base-arabic and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:38<00:00, 13.27it/s, loss=0.396] \nEpoch 2: 100%|██████████| 514/514 [00:38<00:00, 13.28it/s, loss=0.0503] \nEpoch 3: 100%|██████████| 514/514 [00:38<00:00, 13.22it/s, loss=0.0161]  \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 2)\n              precision    recall  f1-score   support\n\n           0     0.6640    0.9007    0.7644       272\n           1     0.9590    0.8360    0.8933       756\n\n    accuracy                         0.8531      1028\n   macro avg     0.8115    0.8684    0.8289      1028\nweighted avg     0.8810    0.8531    0.8592      1028\n\nConfusion Matrix (Fold 2)\n[[245  27]\n [124 632]]\n\n==============================\nFOLD 3\n==============================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bashar-talafha/multi-dialect-bert-base-arabic and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:38<00:00, 13.23it/s, loss=0.28]  \nEpoch 2: 100%|██████████| 514/514 [00:38<00:00, 13.25it/s, loss=0.172]  \nEpoch 3: 100%|██████████| 514/514 [00:38<00:00, 13.27it/s, loss=0.00743] \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 3)\n              precision    recall  f1-score   support\n\n           0     0.8475    0.7326    0.7859       273\n           1     0.9078    0.9523    0.9295       755\n\n    accuracy                         0.8940      1028\n   macro avg     0.8776    0.8425    0.8577      1028\nweighted avg     0.8918    0.8940    0.8914      1028\n\nConfusion Matrix (Fold 3)\n[[200  73]\n [ 36 719]]\n\n==============================\nFOLD 4\n==============================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bashar-talafha/multi-dialect-bert-base-arabic and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:38<00:00, 13.24it/s, loss=0.307] \nEpoch 2: 100%|██████████| 514/514 [00:39<00:00, 13.15it/s, loss=0.0218] \nEpoch 3: 100%|██████████| 514/514 [00:39<00:00, 13.09it/s, loss=0.0272]  \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 4)\n              precision    recall  f1-score   support\n\n           0     0.8139    0.6912    0.7475       272\n           1     0.8945    0.9430    0.9181       755\n\n    accuracy                         0.8763      1027\n   macro avg     0.8542    0.8171    0.8328      1027\nweighted avg     0.8731    0.8763    0.8729      1027\n\nConfusion Matrix (Fold 4)\n[[188  84]\n [ 43 712]]\n\n==============================\nFOLD 5\n==============================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bashar-talafha/multi-dialect-bert-base-arabic and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:39<00:00, 13.16it/s, loss=0.0668]\nEpoch 2: 100%|██████████| 514/514 [00:38<00:00, 13.23it/s, loss=0.037]  \nEpoch 3: 100%|██████████| 514/514 [00:38<00:00, 13.24it/s, loss=0.115]   \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 5)\n              precision    recall  f1-score   support\n\n           0     0.7864    0.8529    0.8183       272\n           1     0.9454    0.9166    0.9307       755\n\n    accuracy                         0.8997      1027\n   macro avg     0.8659    0.8847    0.8745      1027\nweighted avg     0.9033    0.8997    0.9010      1027\n\nConfusion Matrix (Fold 5)\n[[232  40]\n [ 63 692]]\n\n✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\nOVERALL RESULTS (ALL FOLDS)\nbashar-talafha/multi-dialect-bert-base-arabic\n✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\n\nFinal Classification Report\n              precision    recall  f1-score   support\n\n           0     0.7596    0.8170    0.7873      1361\n           1     0.9322    0.9068    0.9193      3777\n\n    accuracy                         0.8830      5138\n   macro avg     0.8459    0.8619    0.8533      5138\nweighted avg     0.8865    0.8830    0.8844      5138\n\nFinal Confusion Matrix\n[[1112  249]\n [ 352 3425]]\n\n✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/526 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27084ad03d4341bdb571c61cec765db3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b157f412878d4093ac363928256b07e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed97d44f6a8646c0a404994ed177bbec"}},"metadata":{}},{"name":"stdout","text":"\n==============================\nFOLD 1\n==============================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5618af865e9e432d848e2042fa9c02c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"857c0c8bd0524c4c81b4af200032839b"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:34<00:00, 14.99it/s, loss=0.549] \nEpoch 2: 100%|██████████| 514/514 [00:34<00:00, 15.08it/s, loss=0.253] \nEpoch 3: 100%|██████████| 514/514 [00:34<00:00, 15.07it/s, loss=0.0254] \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 1)\n              precision    recall  f1-score   support\n\n           0     0.7384    0.8199    0.7770       272\n           1     0.9325    0.8955    0.9136       756\n\n    accuracy                         0.8755      1028\n   macro avg     0.8355    0.8577    0.8453      1028\nweighted avg     0.8812    0.8755    0.8775      1028\n\nConfusion Matrix (Fold 1)\n[[223  49]\n [ 79 677]]\n\n==============================\nFOLD 2\n==============================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:34<00:00, 14.95it/s, loss=0.5]   \nEpoch 2: 100%|██████████| 514/514 [00:34<00:00, 14.89it/s, loss=0.224] \nEpoch 3: 100%|██████████| 514/514 [00:34<00:00, 14.99it/s, loss=0.0407] \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 2)\n              precision    recall  f1-score   support\n\n           0     0.8320    0.7463    0.7868       272\n           1     0.9120    0.9458    0.9286       756\n\n    accuracy                         0.8930      1028\n   macro avg     0.8720    0.8460    0.8577      1028\nweighted avg     0.8908    0.8930    0.8911      1028\n\nConfusion Matrix (Fold 2)\n[[203  69]\n [ 41 715]]\n\n==============================\nFOLD 3\n==============================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:34<00:00, 15.06it/s, loss=0.815] \nEpoch 2: 100%|██████████| 514/514 [00:34<00:00, 15.07it/s, loss=0.169] \nEpoch 3: 100%|██████████| 514/514 [00:34<00:00, 14.89it/s, loss=0.0178] \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 3)\n              precision    recall  f1-score   support\n\n           0     0.7482    0.7509    0.7495       273\n           1     0.9098    0.9086    0.9092       755\n\n    accuracy                         0.8667      1028\n   macro avg     0.8290    0.8298    0.8294      1028\nweighted avg     0.8669    0.8667    0.8668      1028\n\nConfusion Matrix (Fold 3)\n[[205  68]\n [ 69 686]]\n\n==============================\nFOLD 4\n==============================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:34<00:00, 14.86it/s, loss=0.609] \nEpoch 2: 100%|██████████| 514/514 [00:34<00:00, 14.86it/s, loss=0.342] \nEpoch 3: 100%|██████████| 514/514 [00:34<00:00, 14.87it/s, loss=0.0176] \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 4)\n              precision    recall  f1-score   support\n\n           0     0.7241    0.7721    0.7473       272\n           1     0.9159    0.8940    0.9048       755\n\n    accuracy                         0.8617      1027\n   macro avg     0.8200    0.8330    0.8261      1027\nweighted avg     0.8651    0.8617    0.8631      1027\n\nConfusion Matrix (Fold 4)\n[[210  62]\n [ 80 675]]\n\n==============================\nFOLD 5\n==============================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:34<00:00, 14.99it/s, loss=0.23]  \nEpoch 2: 100%|██████████| 514/514 [00:34<00:00, 14.87it/s, loss=0.0229]\nEpoch 3: 100%|██████████| 514/514 [00:34<00:00, 14.93it/s, loss=0.00563]\n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 5)\n              precision    recall  f1-score   support\n\n           0     0.7325    0.8456    0.7850       272\n           1     0.9411    0.8887    0.9142       755\n\n    accuracy                         0.8773      1027\n   macro avg     0.8368    0.8672    0.8496      1027\nweighted avg     0.8858    0.8773    0.8800      1027\n\nConfusion Matrix (Fold 5)\n[[230  42]\n [ 84 671]]\n\n✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\nOVERALL RESULTS (ALL FOLDS)\nsentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\n\nFinal Classification Report\n              precision    recall  f1-score   support\n\n           0     0.7521    0.7869    0.7691      1361\n           1     0.9219    0.9065    0.9142      3777\n\n    accuracy                         0.8749      5138\n   macro avg     0.8370    0.8467    0.8416      5138\nweighted avg     0.8769    0.8749    0.8757      5138\n\nFinal Confusion Matrix\n[[1071  290]\n [ 353 3424]]\n\n✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da4723a0c9854ee0a8859132b184666e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/589 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52d61fdacabb43c683cb90a3c557be50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93804b8c27494265b6029a7a30d2053c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ea2e8538f2b44a6a192202dbb241016"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"baa9e527cf264daaa27e5ef22961bafb"}},"metadata":{}},{"name":"stdout","text":"\n==============================\nFOLD 1\n==============================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/539M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb3d433113464115827e55c960142839"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/distilbert-multilingual-nli-stsb-quora-ranking and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:28<00:00, 17.99it/s, loss=0.862] \nEpoch 2: 100%|██████████| 514/514 [00:28<00:00, 18.13it/s, loss=0.327] \nEpoch 3: 100%|██████████| 514/514 [00:28<00:00, 18.22it/s, loss=0.481]  \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 1)\n              precision    recall  f1-score   support\n\n           0     0.7311    0.8199    0.7730       272\n           1     0.9322    0.8915    0.9114       756\n\n    accuracy                         0.8726      1028\n   macro avg     0.8317    0.8557    0.8422      1028\nweighted avg     0.8790    0.8726    0.8748      1028\n\nConfusion Matrix (Fold 1)\n[[223  49]\n [ 82 674]]\n\n==============================\nFOLD 2\n==============================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/distilbert-multilingual-nli-stsb-quora-ranking and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:28<00:00, 18.18it/s, loss=0.791] \nEpoch 2: 100%|██████████| 514/514 [00:28<00:00, 18.22it/s, loss=0.544]  \nEpoch 3: 100%|██████████| 514/514 [00:28<00:00, 18.23it/s, loss=0.0105] \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 2)\n              precision    recall  f1-score   support\n\n           0     0.6930    0.8382    0.7587       272\n           1     0.9371    0.8664    0.9003       756\n\n    accuracy                         0.8589      1028\n   macro avg     0.8150    0.8523    0.8295      1028\nweighted avg     0.8725    0.8589    0.8629      1028\n\nConfusion Matrix (Fold 2)\n[[228  44]\n [101 655]]\n\n==============================\nFOLD 3\n==============================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/distilbert-multilingual-nli-stsb-quora-ranking and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:28<00:00, 18.21it/s, loss=0.945] \nEpoch 2: 100%|██████████| 514/514 [00:28<00:00, 18.20it/s, loss=0.766] \nEpoch 3: 100%|██████████| 514/514 [00:28<00:00, 18.24it/s, loss=0.579]  \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 3)\n              precision    recall  f1-score   support\n\n           0     0.7251    0.7729    0.7482       273\n           1     0.9159    0.8940    0.9048       755\n\n    accuracy                         0.8619      1028\n   macro avg     0.8205    0.8335    0.8265      1028\nweighted avg     0.8652    0.8619    0.8632      1028\n\nConfusion Matrix (Fold 3)\n[[211  62]\n [ 80 675]]\n\n==============================\nFOLD 4\n==============================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/distilbert-multilingual-nli-stsb-quora-ranking and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:28<00:00, 18.17it/s, loss=0.663] \nEpoch 2: 100%|██████████| 514/514 [00:28<00:00, 18.19it/s, loss=0.596] \nEpoch 3: 100%|██████████| 514/514 [00:28<00:00, 18.23it/s, loss=0.131]  \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 4)\n              precision    recall  f1-score   support\n\n           0     0.6941    0.7757    0.7326       272\n           1     0.9156    0.8768    0.8958       755\n\n    accuracy                         0.8500      1027\n   macro avg     0.8049    0.8263    0.8142      1027\nweighted avg     0.8570    0.8500    0.8526      1027\n\nConfusion Matrix (Fold 4)\n[[211  61]\n [ 93 662]]\n\n==============================\nFOLD 5\n==============================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/distilbert-multilingual-nli-stsb-quora-ranking and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:28<00:00, 18.15it/s, loss=0.166] \nEpoch 2: 100%|██████████| 514/514 [00:28<00:00, 18.21it/s, loss=0.227] \nEpoch 3: 100%|██████████| 514/514 [00:28<00:00, 18.22it/s, loss=0.0399] \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 5)\n              precision    recall  f1-score   support\n\n           0     0.7747    0.7206    0.7467       272\n           1     0.9018    0.9245    0.9130       755\n\n    accuracy                         0.8705      1027\n   macro avg     0.8383    0.8225    0.8298      1027\nweighted avg     0.8681    0.8705    0.8690      1027\n\nConfusion Matrix (Fold 5)\n[[196  76]\n [ 57 698]]\n\n✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\nOVERALL RESULTS (ALL FOLDS)\nsentence-transformers/distilbert-multilingual-nli-stsb-quora-ranking\n✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\n\nFinal Classification Report\n              precision    recall  f1-score   support\n\n           0     0.7213    0.7855    0.7520      1361\n           1     0.9201    0.8907    0.9052      3777\n\n    accuracy                         0.8628      5138\n   macro avg     0.8207    0.8381    0.8286      5138\nweighted avg     0.8675    0.8628    0.8646      5138\n\nFinal Confusion Matrix\n[[1069  292]\n [ 413 3364]]\n\n✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/528 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec6d8faf0efa458a9592a5dce27ed5b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/607 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21aeca8b9afa4e95b266bfab072e92e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc6ead10d2a64f68917e305366a00b57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38497975014c4883b952428d6512f125"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f94dad961b074bccbe40793f2f88bfe0"}},"metadata":{}},{"name":"stdout","text":"\n==============================\nFOLD 1\n==============================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/539M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96aaf946b2af43a1a0021abc4885070a"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/distiluse-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:28<00:00, 18.20it/s, loss=0.653] \nEpoch 2: 100%|██████████| 514/514 [00:28<00:00, 18.25it/s, loss=0.453] \nEpoch 3: 100%|██████████| 514/514 [00:28<00:00, 18.30it/s, loss=0.441] \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 1)\n              precision    recall  f1-score   support\n\n           0     0.7127    0.7022    0.7074       272\n           1     0.8934    0.8981    0.8958       756\n\n    accuracy                         0.8463      1028\n   macro avg     0.8031    0.8002    0.8016      1028\nweighted avg     0.8456    0.8463    0.8459      1028\n\nConfusion Matrix (Fold 1)\n[[191  81]\n [ 77 679]]\n\n==============================\nFOLD 2\n==============================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/distiluse-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:28<00:00, 18.27it/s, loss=0.67]  \nEpoch 2: 100%|██████████| 514/514 [00:28<00:00, 18.28it/s, loss=0.38]  \nEpoch 3: 100%|██████████| 514/514 [00:28<00:00, 18.31it/s, loss=0.186]  \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 2)\n              precision    recall  f1-score   support\n\n           0     0.7208    0.7500    0.7351       272\n           1     0.9087    0.8955    0.9021       756\n\n    accuracy                         0.8570      1028\n   macro avg     0.8148    0.8228    0.8186      1028\nweighted avg     0.8590    0.8570    0.8579      1028\n\nConfusion Matrix (Fold 2)\n[[204  68]\n [ 79 677]]\n\n==============================\nFOLD 3\n==============================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/distiluse-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:28<00:00, 18.29it/s, loss=0.688] \nEpoch 2: 100%|██████████| 514/514 [00:28<00:00, 18.27it/s, loss=0.119] \nEpoch 3: 100%|██████████| 514/514 [00:28<00:00, 18.29it/s, loss=0.0628] \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 3)\n              precision    recall  f1-score   support\n\n           0     0.7723    0.5714    0.6568       273\n           1     0.8584    0.9391    0.8969       755\n\n    accuracy                         0.8414      1028\n   macro avg     0.8153    0.7553    0.7769      1028\nweighted avg     0.8355    0.8414    0.8331      1028\n\nConfusion Matrix (Fold 3)\n[[156 117]\n [ 46 709]]\n\n==============================\nFOLD 4\n==============================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/distiluse-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:28<00:00, 18.27it/s, loss=0.508] \nEpoch 2: 100%|██████████| 514/514 [00:28<00:00, 18.29it/s, loss=0.394] \nEpoch 3: 100%|██████████| 514/514 [00:28<00:00, 18.32it/s, loss=0.0895] \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 4)\n              precision    recall  f1-score   support\n\n           0     0.7297    0.6949    0.7119       272\n           1     0.8919    0.9073    0.8995       755\n\n    accuracy                         0.8510      1027\n   macro avg     0.8108    0.8011    0.8057      1027\nweighted avg     0.8490    0.8510    0.8498      1027\n\nConfusion Matrix (Fold 4)\n[[189  83]\n [ 70 685]]\n\n==============================\nFOLD 5\n==============================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/distiluse-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:28<00:00, 18.33it/s, loss=0.303] \nEpoch 2: 100%|██████████| 514/514 [00:28<00:00, 18.30it/s, loss=0.188] \nEpoch 3: 100%|██████████| 514/514 [00:28<00:00, 18.29it/s, loss=0.0173] \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 5)\n              precision    recall  f1-score   support\n\n           0     0.6839    0.7794    0.7285       272\n           1     0.9163    0.8702    0.8927       755\n\n    accuracy                         0.8462      1027\n   macro avg     0.8001    0.8248    0.8106      1027\nweighted avg     0.8548    0.8462    0.8492      1027\n\nConfusion Matrix (Fold 5)\n[[212  60]\n [ 98 657]]\n\n✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\nOVERALL RESULTS (ALL FOLDS)\nsentence-transformers/distiluse-base-multilingual-cased\n✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\n\nFinal Classification Report\n              precision    recall  f1-score   support\n\n           0     0.7201    0.6995    0.7097      1361\n           1     0.8928    0.9020    0.8974      3777\n\n    accuracy                         0.8484      5138\n   macro avg     0.8065    0.8008    0.8035      5138\nweighted avg     0.8471    0.8484    0.8477      5138\n\nFinal Confusion Matrix\n[[ 952  409]\n [ 370 3407]]\n\n✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/371 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc4e3bc96eb34d2dae913d656be3679b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/572 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5530ff3962c423f93f80680450af3e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3746d0734ba24a77850739b7315cb187"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81dbecf62cfb4f2da51421907502f988"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"539f8d007ec94e05b4c68fc0ecae18ac"}},"metadata":{}},{"name":"stdout","text":"\n==============================\nFOLD 1\n==============================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/539M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ca14a23b8a3447d948d13cbed40b91d"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/clip-ViT-B-32-multilingual-v1 and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:28<00:00, 18.21it/s, loss=0.724] \nEpoch 2: 100%|██████████| 514/514 [00:28<00:00, 18.19it/s, loss=0.757]  \nEpoch 3: 100%|██████████| 514/514 [00:28<00:00, 18.29it/s, loss=0.638]  \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 1)\n              precision    recall  f1-score   support\n\n           0     0.7374    0.8051    0.7698       272\n           1     0.9275    0.8968    0.9119       756\n\n    accuracy                         0.8726      1028\n   macro avg     0.8324    0.8510    0.8408      1028\nweighted avg     0.8772    0.8726    0.8743      1028\n\nConfusion Matrix (Fold 1)\n[[219  53]\n [ 78 678]]\n\n==============================\nFOLD 2\n==============================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/clip-ViT-B-32-multilingual-v1 and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:28<00:00, 18.23it/s, loss=0.683] \nEpoch 2: 100%|██████████| 514/514 [00:28<00:00, 18.22it/s, loss=0.313] \nEpoch 3: 100%|██████████| 514/514 [00:28<00:00, 18.26it/s, loss=0.0113] \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 2)\n              precision    recall  f1-score   support\n\n           0     0.7358    0.8603    0.7932       272\n           1     0.9465    0.8889    0.9168       756\n\n    accuracy                         0.8813      1028\n   macro avg     0.8412    0.8746    0.8550      1028\nweighted avg     0.8907    0.8813    0.8841      1028\n\nConfusion Matrix (Fold 2)\n[[234  38]\n [ 84 672]]\n\n==============================\nFOLD 3\n==============================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/clip-ViT-B-32-multilingual-v1 and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:28<00:00, 18.18it/s, loss=0.614] \nEpoch 2: 100%|██████████| 514/514 [00:28<00:00, 18.19it/s, loss=0.119]  \nEpoch 3: 100%|██████████| 514/514 [00:28<00:00, 18.17it/s, loss=0.111]  \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 3)\n              precision    recall  f1-score   support\n\n           0     0.8512    0.6703    0.7500       273\n           1     0.8893    0.9576    0.9222       755\n\n    accuracy                         0.8813      1028\n   macro avg     0.8702    0.8140    0.8361      1028\nweighted avg     0.8792    0.8813    0.8765      1028\n\nConfusion Matrix (Fold 3)\n[[183  90]\n [ 32 723]]\n\n==============================\nFOLD 4\n==============================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/clip-ViT-B-32-multilingual-v1 and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:28<00:00, 18.20it/s, loss=0.534] \nEpoch 2: 100%|██████████| 514/514 [00:28<00:00, 18.21it/s, loss=0.329] \nEpoch 3: 100%|██████████| 514/514 [00:28<00:00, 18.32it/s, loss=0.0753] \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 4)\n              precision    recall  f1-score   support\n\n           0     0.7222    0.7647    0.7429       272\n           1     0.9134    0.8940    0.9036       755\n\n    accuracy                         0.8598      1027\n   macro avg     0.8178    0.8294    0.8232      1027\nweighted avg     0.8628    0.8598    0.8610      1027\n\nConfusion Matrix (Fold 4)\n[[208  64]\n [ 80 675]]\n\n==============================\nFOLD 5\n==============================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/clip-ViT-B-32-multilingual-v1 and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:28<00:00, 18.32it/s, loss=0.294] \nEpoch 2: 100%|██████████| 514/514 [00:28<00:00, 18.29it/s, loss=0.0654]\nEpoch 3: 100%|██████████| 514/514 [00:28<00:00, 18.25it/s, loss=0.0811] \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 5)\n              precision    recall  f1-score   support\n\n           0     0.7634    0.7831    0.7731       272\n           1     0.9211    0.9126    0.9168       755\n\n    accuracy                         0.8783      1027\n   macro avg     0.8423    0.8478    0.8450      1027\nweighted avg     0.8794    0.8783    0.8788      1027\n\nConfusion Matrix (Fold 5)\n[[213  59]\n [ 66 689]]\n\n✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\nOVERALL RESULTS (ALL FOLDS)\nsentence-transformers/clip-ViT-B-32-multilingual-v1\n✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\n\nFinal Classification Report\n              precision    recall  f1-score   support\n\n           0     0.7566    0.7766    0.7665      1361\n           1     0.9187    0.9100    0.9143      3777\n\n    accuracy                         0.8747      5138\n   macro avg     0.8377    0.8433    0.8404      5138\nweighted avg     0.8758    0.8747    0.8752      5138\n\nFinal Confusion Matrix\n[[1057  304]\n [ 340 3437]]\n\n✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/452 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4eb2c29336364354a8a877838347e26a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/556 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1a447eb147448e7b1fd06a9a6974129"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"017b250b797e493da2ce609b5c1acafa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f17f8bbd4f24f1cbcf21e8b12c6b520"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f2367dd8f3043448aea47d64f190b11"}},"metadata":{}},{"name":"stdout","text":"\n==============================\nFOLD 1\n==============================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/539M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"505a873468714bcab938c7ac91474b68"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/distiluse-base-multilingual-cased-v1 and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:28<00:00, 17.93it/s, loss=0.537] \nEpoch 2: 100%|██████████| 514/514 [00:28<00:00, 17.94it/s, loss=0.427] \nEpoch 3: 100%|██████████| 514/514 [00:28<00:00, 17.99it/s, loss=0.0351] \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 1)\n              precision    recall  f1-score   support\n\n           0     0.7416    0.7279    0.7347       272\n           1     0.9028    0.9087    0.9057       756\n\n    accuracy                         0.8609      1028\n   macro avg     0.8222    0.8183    0.8202      1028\nweighted avg     0.8601    0.8609    0.8605      1028\n\nConfusion Matrix (Fold 1)\n[[198  74]\n [ 69 687]]\n\n==============================\nFOLD 2\n==============================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/distiluse-base-multilingual-cased-v1 and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:28<00:00, 17.96it/s, loss=0.719] \nEpoch 2: 100%|██████████| 514/514 [00:28<00:00, 17.96it/s, loss=0.536] \nEpoch 3: 100%|██████████| 514/514 [00:28<00:00, 18.04it/s, loss=0.174] \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 2)\n              precision    recall  f1-score   support\n\n           0     0.7869    0.7059    0.7442       272\n           1     0.8980    0.9312    0.9143       756\n\n    accuracy                         0.8716      1028\n   macro avg     0.8424    0.8185    0.8292      1028\nweighted avg     0.8686    0.8716    0.8693      1028\n\nConfusion Matrix (Fold 2)\n[[192  80]\n [ 52 704]]\n\n==============================\nFOLD 3\n==============================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/distiluse-base-multilingual-cased-v1 and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:28<00:00, 18.25it/s, loss=0.682] \nEpoch 2: 100%|██████████| 514/514 [00:28<00:00, 18.04it/s, loss=0.0818]\nEpoch 3: 100%|██████████| 514/514 [00:28<00:00, 18.01it/s, loss=0.169]  \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 3)\n              precision    recall  f1-score   support\n\n           0     0.7412    0.6923    0.7159       273\n           1     0.8913    0.9126    0.9018       755\n\n    accuracy                         0.8541      1028\n   macro avg     0.8163    0.8024    0.8089      1028\nweighted avg     0.8515    0.8541    0.8525      1028\n\nConfusion Matrix (Fold 3)\n[[189  84]\n [ 66 689]]\n\n==============================\nFOLD 4\n==============================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/distiluse-base-multilingual-cased-v1 and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:28<00:00, 18.24it/s, loss=0.744] \nEpoch 2: 100%|██████████| 514/514 [00:27<00:00, 18.41it/s, loss=0.417] \nEpoch 3: 100%|██████████| 514/514 [00:27<00:00, 18.39it/s, loss=0.0643]\n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 4)\n              precision    recall  f1-score   support\n\n           0     0.6939    0.7500    0.7208       272\n           1     0.9072    0.8808    0.8938       755\n\n    accuracy                         0.8462      1027\n   macro avg     0.8006    0.8154    0.8073      1027\nweighted avg     0.8507    0.8462    0.8480      1027\n\nConfusion Matrix (Fold 4)\n[[204  68]\n [ 90 665]]\n\n==============================\nFOLD 5\n==============================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/distiluse-base-multilingual-cased-v1 and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:28<00:00, 18.20it/s, loss=0.217] \nEpoch 2: 100%|██████████| 514/514 [00:28<00:00, 18.30it/s, loss=0.0466]\nEpoch 3: 100%|██████████| 514/514 [00:27<00:00, 18.36it/s, loss=0.039] \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 5)\n              precision    recall  f1-score   support\n\n           0     0.7345    0.7426    0.7386       272\n           1     0.9069    0.9033    0.9051       755\n\n    accuracy                         0.8608      1027\n   macro avg     0.8207    0.8230    0.8218      1027\nweighted avg     0.8613    0.8608    0.8610      1027\n\nConfusion Matrix (Fold 5)\n[[202  70]\n [ 73 682]]\n\n✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\nOVERALL RESULTS (ALL FOLDS)\nsentence-transformers/distiluse-base-multilingual-cased-v1\n✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\n\nFinal Classification Report\n              precision    recall  f1-score   support\n\n           0     0.7378    0.7237    0.7307      1361\n           1     0.9011    0.9073    0.9042      3777\n\n    accuracy                         0.8587      5138\n   macro avg     0.8195    0.8155    0.8175      5138\nweighted avg     0.8579    0.8587    0.8583      5138\n\nFinal Confusion Matrix\n[[ 985  376]\n [ 350 3427]]\n\n✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/402 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"443ce0c560af4579864acbba236f2419"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/723 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a343f7db401440c19563621fa6c8de6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8436f9820ae413f82233a46b0b18df5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"291c6ede819d4ef7a393db6a69cabffc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdeb604cdf904f4c9122e30a95db45b7"}},"metadata":{}},{"name":"stdout","text":"\n==============================\nFOLD 1\n==============================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33c0d857db944dcd8aba239aaef218ff"}},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/paraphrase-multilingual-mpnet-base-v2 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:54<00:00,  9.35it/s, loss=0.999] \nEpoch 2: 100%|██████████| 514/514 [00:54<00:00,  9.40it/s, loss=0.609] \nEpoch 3: 100%|██████████| 514/514 [00:54<00:00,  9.38it/s, loss=0.0832] \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 1)\n              precision    recall  f1-score   support\n\n           0     0.8837    0.6985    0.7803       272\n           1     0.8991    0.9669    0.9318       756\n\n    accuracy                         0.8959      1028\n   macro avg     0.8914    0.8327    0.8560      1028\nweighted avg     0.8951    0.8959    0.8917      1028\n\nConfusion Matrix (Fold 1)\n[[190  82]\n [ 25 731]]\n\n==============================\nFOLD 2\n==============================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/paraphrase-multilingual-mpnet-base-v2 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:54<00:00,  9.38it/s, loss=0.324] \nEpoch 2: 100%|██████████| 514/514 [00:54<00:00,  9.38it/s, loss=0.254]  \nEpoch 3: 100%|██████████| 514/514 [00:55<00:00,  9.28it/s, loss=0.0117] \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 2)\n              precision    recall  f1-score   support\n\n           0     0.8514    0.6949    0.7652       272\n           1     0.8970    0.9563    0.9257       756\n\n    accuracy                         0.8872      1028\n   macro avg     0.8742    0.8256    0.8455      1028\nweighted avg     0.8849    0.8872    0.8833      1028\n\nConfusion Matrix (Fold 2)\n[[189  83]\n [ 33 723]]\n\n==============================\nFOLD 3\n==============================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/paraphrase-multilingual-mpnet-base-v2 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:55<00:00,  9.23it/s, loss=0.376] \nEpoch 2: 100%|██████████| 514/514 [00:55<00:00,  9.24it/s, loss=0.112]  \nEpoch 3: 100%|██████████| 514/514 [00:55<00:00,  9.26it/s, loss=0.0179] \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 3)\n              precision    recall  f1-score   support\n\n           0     0.8160    0.7473    0.7801       273\n           1     0.9113    0.9391    0.9250       755\n\n    accuracy                         0.8881      1028\n   macro avg     0.8637    0.8432    0.8525      1028\nweighted avg     0.8860    0.8881    0.8865      1028\n\nConfusion Matrix (Fold 3)\n[[204  69]\n [ 46 709]]\n\n==============================\nFOLD 4\n==============================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/paraphrase-multilingual-mpnet-base-v2 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:55<00:00,  9.23it/s, loss=0.352] \nEpoch 2: 100%|██████████| 514/514 [00:55<00:00,  9.30it/s, loss=0.569]  \nEpoch 3: 100%|██████████| 514/514 [00:54<00:00,  9.37it/s, loss=0.0199] \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 4)\n              precision    recall  f1-score   support\n\n           0     0.7331    0.7978    0.7641       272\n           1     0.9248    0.8954    0.9098       755\n\n    accuracy                         0.8695      1027\n   macro avg     0.8289    0.8466    0.8370      1027\nweighted avg     0.8740    0.8695    0.8712      1027\n\nConfusion Matrix (Fold 4)\n[[217  55]\n [ 79 676]]\n\n==============================\nFOLD 5\n==============================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/paraphrase-multilingual-mpnet-base-v2 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 514/514 [00:54<00:00,  9.36it/s, loss=0.0941]\nEpoch 2: 100%|██████████| 514/514 [00:55<00:00,  9.32it/s, loss=0.0213] \nEpoch 3: 100%|██████████| 514/514 [00:54<00:00,  9.36it/s, loss=0.0312] \n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Fold 5)\n              precision    recall  f1-score   support\n\n           0     0.8008    0.7096    0.7524       272\n           1     0.8995    0.9364    0.9176       755\n\n    accuracy                         0.8763      1027\n   macro avg     0.8502    0.8230    0.8350      1027\nweighted avg     0.8734    0.8763    0.8738      1027\n\nConfusion Matrix (Fold 5)\n[[193  79]\n [ 48 707]]\n\n✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\nOVERALL RESULTS (ALL FOLDS)\nsentence-transformers/paraphrase-multilingual-mpnet-base-v2\n✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\n\nFinal Classification Report\n              precision    recall  f1-score   support\n\n           0     0.8113    0.7296    0.7683      1361\n           1     0.9060    0.9388    0.9221      3777\n\n    accuracy                         0.8834      5138\n   macro avg     0.8586    0.8342    0.8452      5138\nweighted avg     0.8809    0.8834    0.8814      5138\n\nFinal Confusion Matrix\n[[ 993  368]\n [ 231 3546]]\n\n✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\n","output_type":"stream"}],"execution_count":1}]}