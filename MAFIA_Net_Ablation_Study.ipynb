{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11118864,"sourceType":"datasetVersion","datasetId":6933281}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"HEADS 4 4 stage without eff + vit32","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models, transforms\nfrom transformers import AutoTokenizer, AutoModel\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nfrom torchvision.models import resnet50\nimport csv\nimport timm\n\n\nimport os, random, torch, numpy as np\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    ##torch.backends.cudnn.deterministic = False\n    ##torch.backends.cudnn.benchmark = True\n    torch.use_deterministic_algorithms(True, warn_only=False)\n    print(f\"[INFO] All seeds set to {seed}\")\n\nseed_everything(42)\n\ndef seed_worker(worker_id):\n    worker_seed = torch.initial_seed() % 2**32\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)\n\ng = torch.Generator()\ng.manual_seed(42)\n\ndef free_gpu_memory():\n    torch.cuda.empty_cache()\n    torch.cuda.ipc_collect()\n# --------------------- CONFIG ---------------------\n\nTEXT_MODEL_NAME = 'UBC-NLP/MARBERTv2'\nNUM_CLASSES = 2\nBATCH_SIZE = 8\nNUM_EPOCHS = 3\nFOLDS = 5\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndf = pd.read_csv(\"/kaggle/input/ekafnewsforkhawla/shuffled_cleaned_text_file for khawla.txt\", sep=\"\\t\",encoding=\"utf-8\")\nprint(len(df))\n\n\nimage_encoder = models.vit_b_32(weights=models.ViT_B_32_Weights.IMAGENET1K_V1)\n\nimage_encoder.heads = nn.Identity()\n\nclass CrossAttentionLayer(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n        self.norm = nn.LayerNorm(embed_dim)\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, embed_dim),\n            nn.ReLU(),\n            nn.Linear(embed_dim, embed_dim)\n        )\n        self.dropout = nn.Dropout(0.4)\n\n    def forward(self, query, key, value, key_padding_mask=None):\n        attn_output, _ = self.attn(query, key, value, key_padding_mask=key_padding_mask)\n        x = self.norm(query + self.dropout(attn_output))\n        ff_output = self.norm(x + self.dropout(self.ff(x)))\n        return ff_output\n\n# Define the Full Multimodal Classifier\nclass Multimodal_Arabic_Fake_news_Identification_via_Hybrid_Attention_Networks(nn.Module):\n    def __init__(self, text_model_name, image_encoder, num_classes):\n        super().__init__()\n\n        # Text encoder (e.g., BERT)\n        self.text_encoder = AutoModel.from_pretrained(text_model_name)\n        self.text_proj = nn.Linear(self.text_encoder.config.hidden_size, 512)\n\n        # Image encoder (e.g., ResNet, ViT)\n        self.image_encoder = image_encoder  \n        self.image_proj = nn.Linear(768, 512)\n\n        # Cross-modal interaction attention\n        self.cross_modal_attn = CrossAttentionLayer(embed_dim=512, num_heads=4)\n\n        # Final classifier\n        self.classifier = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, num_classes)\n        )\n\n    def forward(self, input_ids, attention_mask, image):\n        # Encode text\n        text_output = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n        text_embeds = self.text_proj(text_output.last_hidden_state)  # (B, L, 512)\n\n        # Encode image\n        image_feats1 = self.image_encoder(image)\n        image_feats = self.image_proj(image_feats1).unsqueeze(1)  # (B, 1, 512)\n        ##visual_guided_text = self.cross_modal_attn(query=image_feats, key=text_embeds, value=text_embeds)\n        # Cross-modal attention - level 1 \n        img_cross = self.cross_modal_attn(query=text_embeds, key=image_feats, value=image_feats)\n        txt_cross = self.cross_modal_attn(query=image_feats, key=text_embeds, value=text_embeds,key_padding_mask=~attention_mask.bool())\n        ##visual_guided_text = self.cross_modal_attn(query=img_cross, key=txt_cross, value=txt_cross)\n\n        # Cross-modal attention - level 2 \n        cross_modal_img2txt = self.cross_modal_attn(query=txt_cross, key=img_cross, value=img_cross)\n        cross_modal_txt2img = self.cross_modal_attn(query=img_cross, key=txt_cross, value=txt_cross)\n        ##visual_guided_text = self.cross_modal_attn(query=cross_modal_img2txt, key=cross_modal_txt2img, value=cross_modal_txt2img)\n\n        # Cross-modal attention - level 3 \n        cross_modal_img2txt1 = self.cross_modal_attn(query=cross_modal_txt2img, key=cross_modal_img2txt, value=cross_modal_img2txt)\n        cross_modal_txt2img1 = self.cross_modal_attn(query=cross_modal_img2txt, key=cross_modal_txt2img, value=cross_modal_txt2img)\n        ##visual_guided_text = self.cross_modal_attn(query=cross_modal_img2txt1, key=cross_modal_txt2img1, value=cross_modal_txt2img1)\n        # Cross-modal attention - level 4   \n        cross_modal_img2txt2 = self.cross_modal_attn(query=cross_modal_txt2img1, key=cross_modal_img2txt1, value=cross_modal_img2txt1)\n        cross_modal_txt2img2 = self.cross_modal_attn(query=cross_modal_img2txt1, key=cross_modal_txt2img1, value=cross_modal_txt2img1)\n\n        # Visual Guided Text attention\n        visual_guided_text = self.cross_modal_attn(query=cross_modal_img2txt2, key=cross_modal_txt2img2, value=cross_modal_txt2img2)\n\n        # Mean pooling\n        final_multi_repr = torch.mean(visual_guided_text, dim=1)  # (B, 512)\n\n        output = self.classifier(final_multi_repr)\n        return output\n\n\n# --------------------- DATASET ---------------------\nclass ArabicMultimodal_Fake_News_Dataset(Dataset):\n    def __init__(self, samples, tokenizer, transform):\n        self.samples = samples\n        self.tokenizer = tokenizer\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        sample = self.samples[idx]\n        ##image = Image.open(sample['image_path']).convert('RGB')\n        image = Image.open(sample[1]).convert('RGB')\n        image = self.transform(image)\n\n        text_enc = self.tokenizer(\n            sample[0], padding='max_length', truncation=True, max_length=64, return_tensors='pt'\n        )\n        return {\n            'input_ids': text_enc['input_ids'].squeeze(0),\n            'attention_mask': text_enc['attention_mask'].squeeze(0),\n            'image': image,\n            'label': torch.tensor(sample[2], dtype=torch.long)\n        }\n\n# --------------------- UTILS ---------------------\n\n\ndef load_data(annotation_file, image_root):\n    samples = []\n    label2folder = {0: 'Fake', 1: 'Real'}\n\n    with open(annotation_file, 'r', encoding='utf-8') as f:\n        reader = csv.DictReader(f, delimiter='\\t')\n        for row in reader:\n            tweet_id = row['id']\n            text = row['preprocess1']\n            label = int(row['label'])\n            labelstr=row['label']\n            dataset = row['dataset']\n\n            folder_name = label2folder[label]\n\n            # Try different extensions\n            image_path = None\n            for ext in ['jpg', 'png', 'jpeg']:\n                temp_path = os.path.join(image_root, folder_name, f\"{tweet_id}.{ext}\")\n                if os.path.exists(temp_path):\n                    image_path = temp_path\n                    break\n\n            if image_path:\n                samples.append((text, image_path, label,dataset))\n            else:\n                print(f\"Image not found for {tweet_id} with label {label}\")\n\n    return samples\n\n# --------------------- TRAIN + EVAL ---------------------\ndef train_model(model, dataloader, optimizer, criterion):\n    model.train()\n    total_loss, total_correct = 0, 0\n    for batch in dataloader:\n        input_ids = batch['input_ids'].to(DEVICE)\n        attention_mask = batch['attention_mask'].to(DEVICE)\n        images = batch['image'].to(DEVICE)\n        labels = batch['label'].to(DEVICE)\n\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask, images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        preds = outputs.argmax(dim=1)\n        total_correct += (preds == labels).sum().item()\n    return total_loss / len(dataloader), total_correct / len(dataloader.dataset)\n\ndef evaluate_model(model, dataloader):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids = batch['input_ids'].to(DEVICE)\n            attention_mask = batch['attention_mask'].to(DEVICE)\n            images = batch['image'].to(DEVICE)\n            labels = batch['label'].to(DEVICE)\n\n            outputs = model(input_ids, attention_mask, images)\n            preds = outputs.argmax(dim=1)\n\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    return all_preds, all_labels\n\n\n# --------------------- MAIN ---------------------\nif __name__ == '__main__':\n    ##label2idx = {'Fake': 1, 'Real': 0}\n    label2idx = {1: 1, 0: 0}  \n    idx2label = {v: k for k, v in label2idx.items()}\n    samples = load_data(\n    '/kaggle/input/ekafnewsforkhawla/shuffled_cleaned_text_file for khawla.txt',\n    '/kaggle/input/ekafnewsforkhawla/sorted_imagesOur')\n    \n    tokenizer = AutoTokenizer.from_pretrained(TEXT_MODEL_NAME)\n    transform = models.ViT_B_32_Weights.IMAGENET1K_V1.transforms()\n    \n    labels = [sample[2] for sample in samples]\n    strat =labels\n    skf = StratifiedKFold(n_splits=FOLDS, shuffle=False, random_state=None)\n    all_preds, all_trues = [], []\n    for fold, (train_idx, val_idx) in enumerate(skf.split(samples, strat)):\n        print(f\"Fold {fold + 1}/{FOLDS}\")\n        train_samples = [samples[i] for i in train_idx]\n        val_samples = [samples[i] for i in val_idx]\n\n        train_dataset = ArabicMultimodal_Fake_News_Dataset(train_samples, tokenizer, transform)\n        val_dataset = ArabicMultimodal_Fake_News_Dataset(val_samples, tokenizer, transform)\n\n        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False,worker_init_fn=seed_worker, generator=g)\n        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,worker_init_fn=seed_worker, generator=g)\n\n        model = Multimodal_Arabic_Fake_news_Identification_via_Hybrid_Attention_Networks(text_model_name=TEXT_MODEL_NAME,image_encoder=image_encoder,num_classes=NUM_CLASSES).to(DEVICE)\n\n\n        ##model = MultiModalFusionModel(TEXT_MODEL_NAME, NUM_CLASSES).to(DEVICE)\n        optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n        criterion = nn.CrossEntropyLoss()\n\n        for epoch in range(NUM_EPOCHS):\n            train_loss, train_acc = train_model(model, train_loader, optimizer, criterion)\n            print(f\"Epoch {epoch + 1}/{NUM_EPOCHS} - Loss: {train_loss:.4f} - Train Acc: {train_acc:.4f}\")\n\n        preds, trues = evaluate_model(model, val_loader)\n        print(f\"Fold {fold + 1} Classification Report:\")\n        print(classification_report(trues, preds, target_names=[\"0\",\"1\"], digits=4))\n        print(\"Confusion Matrix:\")\n        print(confusion_matrix(trues, preds))\n\n        all_preds.extend(preds)\n        all_trues.extend(trues)\n        free_gpu_memory()\n\n    print(\"\\n=== Overall Classification Report ===\")\n    print(TEXT_MODEL_NAME)\n    print(\"\\n=== Overall Classification Report ===\")\n    print(classification_report(all_trues, all_preds, target_names=[\"0\",\"1\"], digits=4))\n    print(\"=== Overall Confusion Matrix ===\")\n    print(confusion_matrix(all_trues, all_preds))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T20:15:37.367886Z","iopub.execute_input":"2026-01-31T20:15:37.368557Z","iopub.status.idle":"2026-01-31T20:52:45.791142Z","shell.execute_reply.started":"2026-01-31T20:15:37.368528Z","shell.execute_reply":"2026-01-31T20:52:45.790468Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[INFO] All seeds set to 42\n5138\nDownloading: \"https://download.pytorch.org/models/vit_b_32-d86f8d99.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_32-d86f8d99.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 337M/337M [00:01<00:00, 188MB/s]  \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/439 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cae5d48ae6534feebe750ced85bff546"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7f8644ad72946699e7ba1f3f669dfe0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6c3c2fb2e1343758608f6f108fa6701"}},"metadata":{}},{"name":"stdout","text":"Fold 1/5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/757 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"925656374c0a42efa427280b9a60c5b4"}},"metadata":{}},{"name":"stderr","text":"2026-01-31 20:16:26.309617: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1769890586.700109      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1769890586.862037      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1769890587.936299      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769890587.936356      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769890587.936359      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769890587.936362      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/654M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"376be169e7a04b4886e1a44ecde04515"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/654M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d14bb36752844d6a667ec701076de52"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3 - Loss: 0.4462 - Train Acc: 0.7881\nEpoch 2/3 - Loss: 0.2146 - Train Acc: 0.9105\nEpoch 3/3 - Loss: 0.0972 - Train Acc: 0.9601\nFold 1 Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.8565    0.7463    0.7976       272\n           1     0.9128    0.9550    0.9334       756\n\n    accuracy                         0.8998      1028\n   macro avg     0.8847    0.8507    0.8655      1028\nweighted avg     0.8979    0.8998    0.8975      1028\n\nConfusion Matrix:\n[[203  69]\n [ 34 722]]\nFold 2/5\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3 - Loss: 0.4114 - Train Acc: 0.8109\nEpoch 2/3 - Loss: 0.2201 - Train Acc: 0.9114\nEpoch 3/3 - Loss: 0.1143 - Train Acc: 0.9606\nFold 2 Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9146    0.6691    0.7728       272\n           1     0.8914    0.9775    0.9325       756\n\n    accuracy                         0.8959      1028\n   macro avg     0.9030    0.8233    0.8527      1028\nweighted avg     0.8976    0.8959    0.8902      1028\n\nConfusion Matrix:\n[[182  90]\n [ 17 739]]\nFold 3/5\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3 - Loss: 0.4394 - Train Acc: 0.7866\nEpoch 2/3 - Loss: 0.2137 - Train Acc: 0.9114\nEpoch 3/3 - Loss: 0.0995 - Train Acc: 0.9659\nFold 3 Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.8692    0.6813    0.7639       273\n           1     0.8931    0.9629    0.9267       755\n\n    accuracy                         0.8881      1028\n   macro avg     0.8811    0.8221    0.8453      1028\nweighted avg     0.8868    0.8881    0.8835      1028\n\nConfusion Matrix:\n[[186  87]\n [ 28 727]]\nFold 4/5\nEpoch 1/3 - Loss: 0.4211 - Train Acc: 0.8066\nEpoch 2/3 - Loss: 0.2056 - Train Acc: 0.9253\nEpoch 3/3 - Loss: 0.1096 - Train Acc: 0.9645\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Fold 4 Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.7860    0.7831    0.7845       272\n           1     0.9220    0.9232    0.9226       755\n\n    accuracy                         0.8861      1027\n   macro avg     0.8540    0.8531    0.8535      1027\nweighted avg     0.8859    0.8861    0.8860      1027\n\nConfusion Matrix:\n[[213  59]\n [ 58 697]]\nFold 5/5\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3 - Loss: 0.4155 - Train Acc: 0.8151\nEpoch 2/3 - Loss: 0.1928 - Train Acc: 0.9241\nEpoch 3/3 - Loss: 0.0832 - Train Acc: 0.9703\nFold 5 Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.8550    0.8235    0.8390       272\n           1     0.9373    0.9497    0.9434       755\n\n    accuracy                         0.9163      1027\n   macro avg     0.8961    0.8866    0.8912      1027\nweighted avg     0.9155    0.9163    0.9158      1027\n\nConfusion Matrix:\n[[224  48]\n [ 38 717]]\n\n=== Overall Classification Report ===\nUBC-NLP/MARBERTv2\n\n=== Overall Classification Report ===\n              precision    recall  f1-score   support\n\n           0     0.8521    0.7406    0.7925      1361\n           1     0.9107    0.9537    0.9317      3777\n\n    accuracy                         0.8972      5138\n   macro avg     0.8814    0.8471    0.8621      5138\nweighted avg     0.8952    0.8972    0.8948      5138\n\n=== Overall Confusion Matrix ===\n[[1008  353]\n [ 175 3602]]\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"HEADS 4 4 stage without Marbert+ DMSB","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models, transforms\nfrom transformers import AutoTokenizer, AutoModel\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nfrom torchvision.models import resnet50\nimport csv\nimport timm\n\n\nimport os, random, torch, numpy as np\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    ##torch.backends.cudnn.deterministic = False\n    ##torch.backends.cudnn.benchmark = True\n    torch.use_deterministic_algorithms(True, warn_only=False)\n    print(f\"[INFO] All seeds set to {seed}\")\n\nseed_everything(42)\n\ndef seed_worker(worker_id):\n    worker_seed = torch.initial_seed() % 2**32\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)\n\ng = torch.Generator()\ng.manual_seed(42)\n\ndef free_gpu_memory():\n    torch.cuda.empty_cache()\n    torch.cuda.ipc_collect()\n# --------------------- CONFIG ---------------------\n\nTEXT_MODEL_NAME = 'sentence-transformers/distilbert-multilingual-nli-stsb-quora-ranking'\nNUM_CLASSES = 2\nBATCH_SIZE = 8\nNUM_EPOCHS = 3\nFOLDS = 5\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndf = pd.read_csv(\"/kaggle/input/ekafnewsforkhawla/shuffled_cleaned_text_file for khawla.txt\", sep=\"\\t\",encoding=\"utf-8\")\nprint(len(df))\n\nimage_encoder = models.efficientnet_b1(weights=models.EfficientNet_B1_Weights.DEFAULT)\n\n\nimage_encoder.classifier = nn.Identity()\n\nclass CrossAttentionLayer(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n        self.norm = nn.LayerNorm(embed_dim)\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, embed_dim),\n            nn.ReLU(),\n            nn.Linear(embed_dim, embed_dim)\n        )\n        self.dropout = nn.Dropout(0.4)\n\n    def forward(self, query, key, value, key_padding_mask=None):\n        attn_output, _ = self.attn(query, key, value, key_padding_mask=key_padding_mask)\n        x = self.norm(query + self.dropout(attn_output))\n        ff_output = self.norm(x + self.dropout(self.ff(x)))\n        return ff_output\n\n# Define the Full Multimodal Classifier\nclass Multimodal_Arabic_Fake_news_Identification_via_Hybrid_Attention_Networks(nn.Module):\n    def __init__(self, text_model_name, image_encoder, num_classes):\n        super().__init__()\n\n        # Text encoder (e.g., BERT)\n        self.text_encoder = AutoModel.from_pretrained(text_model_name)\n        self.text_proj = nn.Linear(self.text_encoder.config.hidden_size, 512)\n\n        # Image encoder (e.g., ResNet, ViT)\n        self.image_encoder = image_encoder \n        self.image_proj = nn.Linear(1280, 512)\n\n        # Cross-modal interaction attention\n        self.cross_modal_attn = CrossAttentionLayer(embed_dim=512, num_heads=4)\n\n        # Final classifier\n        self.classifier = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, num_classes)\n        )\n\n    def forward(self, input_ids, attention_mask, image):\n        # Encode text\n        text_output = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n        text_embeds = self.text_proj(text_output.last_hidden_state)  # (B, L, 512)\n\n        # Encode image\n        image_feats1 = self.image_encoder(image) \n        image_feats = self.image_proj(image_feats1).unsqueeze(1)  # (B, 1, 512)\n        ##visual_guided_text = self.cross_modal_attn(query=image_feats, key=text_embeds, value=text_embeds)\n        # Cross-modal attention - level 1 \n        img_cross = self.cross_modal_attn(query=text_embeds, key=image_feats, value=image_feats)\n        txt_cross = self.cross_modal_attn(query=image_feats, key=text_embeds, value=text_embeds,key_padding_mask=~attention_mask.bool())\n        ##visual_guided_text = self.cross_modal_attn(query=img_cross, key=txt_cross, value=txt_cross)\n\n        # Cross-modal attention - level 2 \n        cross_modal_img2txt = self.cross_modal_attn(query=txt_cross, key=img_cross, value=img_cross)\n        cross_modal_txt2img = self.cross_modal_attn(query=img_cross, key=txt_cross, value=txt_cross)\n        ##visual_guided_text = self.cross_modal_attn(query=cross_modal_img2txt, key=cross_modal_txt2img, value=cross_modal_txt2img)\n\n        # Cross-modal attention - level 3 \n        cross_modal_img2txt1 = self.cross_modal_attn(query=cross_modal_txt2img, key=cross_modal_img2txt, value=cross_modal_img2txt)\n        cross_modal_txt2img1 = self.cross_modal_attn(query=cross_modal_img2txt, key=cross_modal_txt2img, value=cross_modal_txt2img)\n        ##visual_guided_text = self.cross_modal_attn(query=cross_modal_img2txt1, key=cross_modal_txt2img1, value=cross_modal_txt2img1)\n        # Cross-modal attention - level 4   \n        cross_modal_img2txt2 = self.cross_modal_attn(query=cross_modal_txt2img1, key=cross_modal_img2txt1, value=cross_modal_img2txt1)\n        cross_modal_txt2img2 = self.cross_modal_attn(query=cross_modal_img2txt1, key=cross_modal_txt2img1, value=cross_modal_txt2img1)\n\n        # Visual Guided Text attention\n        visual_guided_text = self.cross_modal_attn(query=cross_modal_img2txt2, key=cross_modal_txt2img2, value=cross_modal_txt2img2)\n\n        # Mean pooling\n        final_multi_repr = torch.mean(visual_guided_text, dim=1)  # (B, 512)\n\n\n        output = self.classifier(final_multi_repr)\n        return output\n\n\n# --------------------- DATASET ---------------------\nclass ArabicMultimodal_Fake_News_Dataset(Dataset):\n    def __init__(self, samples, tokenizer, transform):\n        self.samples = samples\n        self.tokenizer = tokenizer\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        sample = self.samples[idx]\n        ##image = Image.open(sample['image_path']).convert('RGB')\n        image = Image.open(sample[1]).convert('RGB')\n        image = self.transform(image)\n\n        text_enc = self.tokenizer(\n            sample[0], padding='max_length', truncation=True, max_length=64, return_tensors='pt'\n        )\n        return {\n            'input_ids': text_enc['input_ids'].squeeze(0),\n            'attention_mask': text_enc['attention_mask'].squeeze(0),\n            'image': image,\n            'label': torch.tensor(sample[2], dtype=torch.long)\n        }\n\n# --------------------- UTILS ---------------------\n\n\ndef load_data(annotation_file, image_root):\n    samples = []\n    label2folder = {0: 'Fake', 1: 'Real'}\n\n    with open(annotation_file, 'r', encoding='utf-8') as f:\n        reader = csv.DictReader(f, delimiter='\\t')\n        for row in reader:\n            tweet_id = row['id']\n            text = row['preprocess1']\n            label = int(row['label'])\n            labelstr=row['label']\n            dataset = row['dataset']\n\n            folder_name = label2folder[label]\n\n            # Try different extensions\n            image_path = None\n            for ext in ['jpg', 'png', 'jpeg']:\n                temp_path = os.path.join(image_root, folder_name, f\"{tweet_id}.{ext}\")\n                if os.path.exists(temp_path):\n                    image_path = temp_path\n                    break\n\n            if image_path:\n                samples.append((text, image_path, label,dataset))\n            else:\n                print(f\"Image not found for {tweet_id} with label {label}\")\n\n    return samples\n\n# --------------------- TRAIN + EVAL ---------------------\ndef train_model(model, dataloader, optimizer, criterion):\n    model.train()\n    total_loss, total_correct = 0, 0\n    for batch in dataloader:\n        input_ids = batch['input_ids'].to(DEVICE)\n        attention_mask = batch['attention_mask'].to(DEVICE)\n        images = batch['image'].to(DEVICE)\n        labels = batch['label'].to(DEVICE)\n\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask, images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        preds = outputs.argmax(dim=1)\n        total_correct += (preds == labels).sum().item()\n    return total_loss / len(dataloader), total_correct / len(dataloader.dataset)\n\ndef evaluate_model(model, dataloader):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids = batch['input_ids'].to(DEVICE)\n            attention_mask = batch['attention_mask'].to(DEVICE)\n            images = batch['image'].to(DEVICE)\n            labels = batch['label'].to(DEVICE)\n\n            outputs = model(input_ids, attention_mask, images)\n            preds = outputs.argmax(dim=1)\n\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    return all_preds, all_labels\n\n\n# --------------------- MAIN ---------------------\nif __name__ == '__main__':\n    ##label2idx = {'Fake': 1, 'Real': 0}\n    label2idx = {1: 1, 0: 0}  \n    idx2label = {v: k for k, v in label2idx.items()}\n    samples = load_data(\n    '/kaggle/input/ekafnewsforkhawla/shuffled_cleaned_text_file for khawla.txt',\n    '/kaggle/input/ekafnewsforkhawla/sorted_imagesOur')\n    \n    tokenizer = AutoTokenizer.from_pretrained(TEXT_MODEL_NAME)\n    transform = models.EfficientNet_B1_Weights.DEFAULT.transforms()\n    \n    labels = [sample[2] for sample in samples]  \n    strat =labels\n    skf = StratifiedKFold(n_splits=FOLDS, shuffle=False, random_state=None)\n    all_preds, all_trues = [], []\n    for fold, (train_idx, val_idx) in enumerate(skf.split(samples, strat)):\n        print(f\"Fold {fold + 1}/{FOLDS}\")\n        train_samples = [samples[i] for i in train_idx]\n        val_samples = [samples[i] for i in val_idx]\n\n        train_dataset = ArabicMultimodal_Fake_News_Dataset(train_samples, tokenizer, transform)\n        val_dataset = ArabicMultimodal_Fake_News_Dataset(val_samples, tokenizer, transform)\n\n        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False,worker_init_fn=seed_worker, generator=g)\n        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,worker_init_fn=seed_worker, generator=g)\n\n        model = Multimodal_Arabic_Fake_news_Identification_via_Hybrid_Attention_Networks(text_model_name=TEXT_MODEL_NAME,image_encoder=image_encoder,num_classes=NUM_CLASSES).to(DEVICE)\n\n\n        ##model = MultiModalFusionModel(TEXT_MODEL_NAME, NUM_CLASSES).to(DEVICE)\n        optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n        criterion = nn.CrossEntropyLoss()\n\n        for epoch in range(NUM_EPOCHS):\n            train_loss, train_acc = train_model(model, train_loader, optimizer, criterion)\n            print(f\"Epoch {epoch + 1}/{NUM_EPOCHS} - Loss: {train_loss:.4f} - Train Acc: {train_acc:.4f}\")\n\n        preds, trues = evaluate_model(model, val_loader)\n        print(f\"Fold {fold + 1} Classification Report:\")\n        print(classification_report(trues, preds, target_names=[\"0\",\"1\"], digits=4))\n        print(\"Confusion Matrix:\")\n        print(confusion_matrix(trues, preds))\n\n        all_preds.extend(preds)\n        all_trues.extend(trues)\n        free_gpu_memory()\n\n    print(\"\\n=== Overall Classification Report ===\")\n    print(TEXT_MODEL_NAME)\n    print(\"\\n=== Overall Classification Report ===\")\n    print(classification_report(all_trues, all_preds, target_names=[\"0\",\"1\"], digits=4))\n    print(\"=== Overall Confusion Matrix ===\")\n    print(confusion_matrix(all_trues, all_preds))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T20:52:45.792808Z","iopub.execute_input":"2026-01-31T20:52:45.793522Z","iopub.status.idle":"2026-01-31T21:26:35.056436Z","shell.execute_reply.started":"2026-01-31T20:52:45.793492Z","shell.execute_reply":"2026-01-31T21:26:35.055751Z"}},"outputs":[{"name":"stdout","text":"[INFO] All seeds set to 42\n5138\nDownloading: \"https://download.pytorch.org/models/efficientnet_b1-c27df63c.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b1-c27df63c.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 30.1M/30.1M [00:00<00:00, 120MB/s] \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0fdc5a7b4b7423995f229bbb0f1a215"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/589 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdbdec3071e74657baeab7e7a1da8164"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa618cfb34694a22b4d26f185bb0181a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb3565d339ee4c84b70d377d7ba400d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2ac32a7c8834836badab18deb1faabe"}},"metadata":{}},{"name":"stdout","text":"Fold 1/5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/539M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da028dce49d74c62b039abe0cbcdc1c3"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3 - Loss: 0.4346 - Train Acc: 0.7983\nEpoch 2/3 - Loss: 0.2340 - Train Acc: 0.9080\nEpoch 3/3 - Loss: 0.1343 - Train Acc: 0.9506\nFold 1 Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.7671    0.8235    0.7943       272\n           1     0.9348    0.9101    0.9223       756\n\n    accuracy                         0.8872      1028\n   macro avg     0.8510    0.8668    0.8583      1028\nweighted avg     0.8904    0.8872    0.8884      1028\n\nConfusion Matrix:\n[[224  48]\n [ 68 688]]\nFold 2/5\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3 - Loss: 0.3903 - Train Acc: 0.8178\nEpoch 2/3 - Loss: 0.1827 - Train Acc: 0.9285\nEpoch 3/3 - Loss: 0.1036 - Train Acc: 0.9618\nFold 2 Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.8379    0.7794    0.8076       272\n           1     0.9226    0.9458    0.9340       756\n\n    accuracy                         0.9018      1028\n   macro avg     0.8803    0.8626    0.8708      1028\nweighted avg     0.9002    0.9018    0.9006      1028\n\nConfusion Matrix:\n[[212  60]\n [ 41 715]]\nFold 3/5\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3 - Loss: 0.3571 - Train Acc: 0.8401\nEpoch 2/3 - Loss: 0.1644 - Train Acc: 0.9394\nEpoch 3/3 - Loss: 0.0859 - Train Acc: 0.9720\nFold 3 Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.7024    0.8645    0.7750       273\n           1     0.9465    0.8675    0.9053       755\n\n    accuracy                         0.8667      1028\n   macro avg     0.8245    0.8660    0.8402      1028\nweighted avg     0.8817    0.8667    0.8707      1028\n\nConfusion Matrix:\n[[236  37]\n [100 655]]\nFold 4/5\nEpoch 1/3 - Loss: 0.3341 - Train Acc: 0.8489\nEpoch 2/3 - Loss: 0.1311 - Train Acc: 0.9509\nEpoch 3/3 - Loss: 0.0845 - Train Acc: 0.9691\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Fold 4 Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.7743    0.9081    0.8359       272\n           1     0.9647    0.9046    0.9337       755\n\n    accuracy                         0.9056      1027\n   macro avg     0.8695    0.9064    0.8848      1027\nweighted avg     0.9143    0.9056    0.9078      1027\n\nConfusion Matrix:\n[[247  25]\n [ 72 683]]\nFold 5/5\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3 - Loss: 0.2632 - Train Acc: 0.8903\nEpoch 2/3 - Loss: 0.1126 - Train Acc: 0.9579\nEpoch 3/3 - Loss: 0.0733 - Train Acc: 0.9752\nFold 5 Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.6897    0.9559    0.8012       272\n           1     0.9815    0.8450    0.9082       755\n\n    accuracy                         0.8744      1027\n   macro avg     0.8356    0.9005    0.8547      1027\nweighted avg     0.9042    0.8744    0.8799      1027\n\nConfusion Matrix:\n[[260  12]\n [117 638]]\n\n=== Overall Classification Report ===\nsentence-transformers/distilbert-multilingual-nli-stsb-quora-ranking\n\n=== Overall Classification Report ===\n              precision    recall  f1-score   support\n\n           0     0.7476    0.8663    0.8026      1361\n           1     0.9489    0.8946    0.9210      3777\n\n    accuracy                         0.8871      5138\n   macro avg     0.8483    0.8805    0.8618      5138\nweighted avg     0.8956    0.8871    0.8896      5138\n\n=== Overall Confusion Matrix ===\n[[1179  182]\n [ 398 3379]]\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"HEADS 4 4 stage without Vgta","metadata":{}},{"cell_type":"code","source":"\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models, transforms\nfrom transformers import AutoTokenizer, AutoModel\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nfrom torchvision.models import resnet50\nimport csv\nimport timm\n\n\nimport os, random, torch, numpy as np\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    ##torch.backends.cudnn.deterministic = False\n    ##torch.backends.cudnn.benchmark = True\n    torch.use_deterministic_algorithms(True, warn_only=False)\n    print(f\"[INFO] All seeds set to {seed}\")\n\nseed_everything(42)\n\ndef seed_worker(worker_id):\n    worker_seed = torch.initial_seed() % 2**32\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)\n\ng = torch.Generator()\ng.manual_seed(42)\n\ndef free_gpu_memory():\n    torch.cuda.empty_cache()\n    torch.cuda.ipc_collect()\n# --------------------- CONFIG ---------------------\n\nTEXT_MODEL_NAME = 'UBC-NLP/MARBERTv2'\nNUM_CLASSES = 2\nBATCH_SIZE = 8\nNUM_EPOCHS = 3\nFOLDS = 5\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndf = pd.read_csv(\"/kaggle/input/ekafnewsforkhawla/shuffled_cleaned_text_file for khawla.txt\", sep=\"\\t\",encoding=\"utf-8\")\nprint(len(df))\n\nimage_encoder = models.efficientnet_b1(weights=models.EfficientNet_B1_Weights.DEFAULT)\n\n\nimage_encoder.classifier = nn.Identity()\n\nclass CrossAttentionLayer(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n        self.norm = nn.LayerNorm(embed_dim)\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, embed_dim),\n            nn.ReLU(),\n            nn.Linear(embed_dim, embed_dim)\n        )\n        self.dropout = nn.Dropout(0.4)\n\n    def forward(self, query, key, value, key_padding_mask=None):\n        attn_output, _ = self.attn(query, key, value, key_padding_mask=key_padding_mask)\n        x = self.norm(query + self.dropout(attn_output))\n        ff_output = self.norm(x + self.dropout(self.ff(x)))\n        return ff_output\n\n# Define the Full Multimodal Classifier\nclass Multimodal_Arabic_Fake_news_Identification_via_Hybrid_Attention_Networks(nn.Module):\n    def __init__(self, text_model_name, image_encoder, num_classes):\n        super().__init__()\n\n        # Text encoder (e.g., BERT)\n        self.text_encoder = AutoModel.from_pretrained(text_model_name)\n        self.text_proj = nn.Linear(self.text_encoder.config.hidden_size, 512)\n\n        # Image encoder (e.g., ResNet, ViT)\n        self.image_encoder = image_encoder  # Must output (B, 2048)\n        ##self.image_proj = nn.Linear(1000, 512)\n        ##self.image_proj = nn.Linear(2560, 512)\n        self.image_proj = nn.Linear(1280, 512)\n        ##self.image_proj = nn.Linear(768, 512)\n\n        # Cross-modal interaction attention\n        self.cross_modal_attn = CrossAttentionLayer(embed_dim=512, num_heads=4)\n\n        # Final classifier\n        self.classifier = nn.Sequential(nn.Linear(1024, 256),nn.ReLU(),nn.Linear(256, num_classes))\n        ############# Without VGTA#######################################\n        ##self.classifier = nn.Sequential(nn.Linear(1024, 256), nn.ReLU(), nn.Linear(256, num_classes))\n    def forward(self, input_ids, attention_mask, image):\n        # Encode text\n        text_output = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n        text_embeds = self.text_proj(text_output.last_hidden_state)  # (B, L, 512)\n\n        # Encode image\n        image_feats1 = self.image_encoder(image) \n        image_feats = self.image_proj(image_feats1).unsqueeze(1)  # (B, 1, 512)\n        ##visual_guided_text = self.cross_modal_attn(query=image_feats, key=text_embeds, value=text_embeds)\n        # Cross-modal attention - level 1 \n        img_cross = self.cross_modal_attn(query=text_embeds, key=image_feats, value=image_feats)\n        txt_cross = self.cross_modal_attn(query=image_feats, key=text_embeds, value=text_embeds,key_padding_mask=~attention_mask.bool())\n        ##visual_guided_text = self.cross_modal_attn(query=img_cross, key=txt_cross, value=txt_cross)\n\n        # Cross-modal attention - level 2 \n        cross_modal_img2txt = self.cross_modal_attn(query=txt_cross, key=img_cross, value=img_cross)\n        cross_modal_txt2img = self.cross_modal_attn(query=img_cross, key=txt_cross, value=txt_cross)\n        ##visual_guided_text = self.cross_modal_attn(query=cross_modal_img2txt, key=cross_modal_txt2img, value=cross_modal_txt2img)\n\n        # Cross-modal attention - level 3 \n        cross_modal_img2txt1 = self.cross_modal_attn(query=cross_modal_txt2img, key=cross_modal_img2txt, value=cross_modal_img2txt)\n        cross_modal_txt2img1 = self.cross_modal_attn(query=cross_modal_img2txt, key=cross_modal_txt2img, value=cross_modal_txt2img)\n        ##visual_guided_text = self.cross_modal_attn(query=cross_modal_img2txt1, key=cross_modal_txt2img1, value=cross_modal_txt2img1)\n        # Cross-modal attention - level 4   \n        cross_modal_img2txt2 = self.cross_modal_attn(query=cross_modal_txt2img1, key=cross_modal_img2txt1, value=cross_modal_img2txt1)\n        cross_modal_txt2img2 = self.cross_modal_attn(query=cross_modal_img2txt1, key=cross_modal_txt2img1, value=cross_modal_txt2img1)\n\n        final_img_repr = torch.mean(cross_modal_img2txt2, dim=1)\n        final_txt_repr = torch.mean(cross_modal_txt2img2, dim=1)\n        final_multi_repr = torch.cat([final_img_repr, final_txt_repr], dim=1)\n\n        output = self.classifier(final_multi_repr)\n        return output\n\n\n# --------------------- DATASET ---------------------\nclass ArabicMultimodal_Fake_News_Dataset(Dataset):\n    def __init__(self, samples, tokenizer, transform):\n        self.samples = samples\n        self.tokenizer = tokenizer\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        sample = self.samples[idx]\n        ##image = Image.open(sample['image_path']).convert('RGB')\n        image = Image.open(sample[1]).convert('RGB')\n        image = self.transform(image)\n\n        text_enc = self.tokenizer(\n            sample[0], padding='max_length', truncation=True, max_length=64, return_tensors='pt'\n        )\n        return {\n            'input_ids': text_enc['input_ids'].squeeze(0),\n            'attention_mask': text_enc['attention_mask'].squeeze(0),\n            'image': image,\n            'label': torch.tensor(sample[2], dtype=torch.long)\n        }\n\n# --------------------- UTILS ---------------------\n\n\ndef load_data(annotation_file, image_root):\n    samples = []\n    label2folder = {0: 'Fake', 1: 'Real'}\n\n    with open(annotation_file, 'r', encoding='utf-8') as f:\n        reader = csv.DictReader(f, delimiter='\\t')\n        for row in reader:\n            tweet_id = row['id']\n            text = row['preprocess1']\n            label = int(row['label'])\n            labelstr=row['label']\n            dataset = row['dataset']\n\n            folder_name = label2folder[label]\n\n            # Try different extensions\n            image_path = None\n            for ext in ['jpg', 'png', 'jpeg']:\n                temp_path = os.path.join(image_root, folder_name, f\"{tweet_id}.{ext}\")\n                if os.path.exists(temp_path):\n                    image_path = temp_path\n                    break\n\n            if image_path:\n                samples.append((text, image_path, label,dataset))\n            else:\n                print(f\"Image not found for {tweet_id} with label {label}\")\n\n    return samples\n\n# --------------------- TRAIN + EVAL ---------------------\ndef train_model(model, dataloader, optimizer, criterion):\n    model.train()\n    total_loss, total_correct = 0, 0\n    for batch in dataloader:\n        input_ids = batch['input_ids'].to(DEVICE)\n        attention_mask = batch['attention_mask'].to(DEVICE)\n        images = batch['image'].to(DEVICE)\n        labels = batch['label'].to(DEVICE)\n\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask, images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        preds = outputs.argmax(dim=1)\n        total_correct += (preds == labels).sum().item()\n    return total_loss / len(dataloader), total_correct / len(dataloader.dataset)\n\ndef evaluate_model(model, dataloader):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids = batch['input_ids'].to(DEVICE)\n            attention_mask = batch['attention_mask'].to(DEVICE)\n            images = batch['image'].to(DEVICE)\n            labels = batch['label'].to(DEVICE)\n\n            outputs = model(input_ids, attention_mask, images)\n            preds = outputs.argmax(dim=1)\n\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    return all_preds, all_labels\n\n\n# --------------------- MAIN ---------------------\nif __name__ == '__main__':\n    ##label2idx = {'Fake': 1, 'Real': 0}\n    label2idx = {1: 1, 0: 0}  \n    idx2label = {v: k for k, v in label2idx.items()}\n    samples = load_data(\n    '/kaggle/input/ekafnewsforkhawla/shuffled_cleaned_text_file for khawla.txt',\n    '/kaggle/input/ekafnewsforkhawla/sorted_imagesOur')\n    \n    tokenizer = AutoTokenizer.from_pretrained(TEXT_MODEL_NAME)\n    transform = models.EfficientNet_B1_Weights.DEFAULT.transforms()\n    \n    labels = [sample[2] for sample in samples]\n    ##print(labels)\n    \n    strat =labels\n    skf = StratifiedKFold(n_splits=FOLDS, shuffle=False, random_state=None)\n    all_preds, all_trues = [], []\n    for fold, (train_idx, val_idx) in enumerate(skf.split(samples, strat)):\n        print(f\"Fold {fold + 1}/{FOLDS}\")\n        train_samples = [samples[i] for i in train_idx]\n        val_samples = [samples[i] for i in val_idx]\n\n        train_dataset = ArabicMultimodal_Fake_News_Dataset(train_samples, tokenizer, transform)\n        val_dataset = ArabicMultimodal_Fake_News_Dataset(val_samples, tokenizer, transform)\n\n        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False,worker_init_fn=seed_worker, generator=g)\n        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,worker_init_fn=seed_worker, generator=g)\n\n        model = Multimodal_Arabic_Fake_news_Identification_via_Hybrid_Attention_Networks(text_model_name=TEXT_MODEL_NAME,image_encoder=image_encoder,num_classes=NUM_CLASSES).to(DEVICE)\n\n\n        ##model = MultiModalFusionModel(TEXT_MODEL_NAME, NUM_CLASSES).to(DEVICE)\n        optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n        criterion = nn.CrossEntropyLoss()\n\n        for epoch in range(NUM_EPOCHS):\n            train_loss, train_acc = train_model(model, train_loader, optimizer, criterion)\n            print(f\"Epoch {epoch + 1}/{NUM_EPOCHS} - Loss: {train_loss:.4f} - Train Acc: {train_acc:.4f}\")\n\n        preds, trues = evaluate_model(model, val_loader)\n        print(f\"Fold {fold + 1} Classification Report:\")\n        print(classification_report(trues, preds, target_names=[\"0\",\"1\"], digits=4))\n        print(\"Confusion Matrix:\")\n        print(confusion_matrix(trues, preds))\n\n        all_preds.extend(preds)\n        all_trues.extend(trues)\n        free_gpu_memory()\n\n    print(\"\\n=== Overall Classification Report ===\")\n    print(TEXT_MODEL_NAME)\n    print(\"\\n=== Overall Classification Report ===\")\n    print(classification_report(all_trues, all_preds, target_names=[\"0\",\"1\"], digits=4))\n    print(\"=== Overall Confusion Matrix ===\")\n    print(confusion_matrix(all_trues, all_preds))\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T21:26:35.057568Z","iopub.execute_input":"2026-01-31T21:26:35.057934Z","iopub.status.idle":"2026-01-31T22:03:33.050957Z","shell.execute_reply.started":"2026-01-31T21:26:35.057908Z","shell.execute_reply":"2026-01-31T22:03:33.050240Z"}},"outputs":[{"name":"stdout","text":"[INFO] All seeds set to 42\n5138\nFold 1/5\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3 - Loss: 0.4346 - Train Acc: 0.7944\nEpoch 2/3 - Loss: 0.1971 - Train Acc: 0.9141\nEpoch 3/3 - Loss: 0.0842 - Train Acc: 0.9674\nFold 1 Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.7827    0.9007    0.8376       272\n           1     0.9622    0.9101    0.9354       756\n\n    accuracy                         0.9076      1028\n   macro avg     0.8725    0.9054    0.8865      1028\nweighted avg     0.9147    0.9076    0.9095      1028\n\nConfusion Matrix:\n[[245  27]\n [ 68 688]]\nFold 2/5\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3 - Loss: 0.4445 - Train Acc: 0.7878\nEpoch 2/3 - Loss: 0.1792 - Train Acc: 0.9285\nEpoch 3/3 - Loss: 0.0799 - Train Acc: 0.9727\nFold 2 Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.7343    0.9044    0.8105       272\n           1     0.9625    0.8823    0.9206       756\n\n    accuracy                         0.8881      1028\n   macro avg     0.8484    0.8933    0.8656      1028\nweighted avg     0.9021    0.8881    0.8915      1028\n\nConfusion Matrix:\n[[246  26]\n [ 89 667]]\nFold 3/5\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3 - Loss: 0.3397 - Train Acc: 0.8523\nEpoch 2/3 - Loss: 0.1128 - Train Acc: 0.9584\nEpoch 3/3 - Loss: 0.0606 - Train Acc: 0.9798\nFold 3 Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.8792    0.7729    0.8226       273\n           1     0.9213    0.9616    0.9410       755\n\n    accuracy                         0.9115      1028\n   macro avg     0.9002    0.8672    0.8818      1028\nweighted avg     0.9101    0.9115    0.9096      1028\n\nConfusion Matrix:\n[[211  62]\n [ 29 726]]\nFold 4/5\nEpoch 1/3 - Loss: 0.3055 - Train Acc: 0.8648\nEpoch 2/3 - Loss: 0.0967 - Train Acc: 0.9652\nEpoch 3/3 - Loss: 0.0520 - Train Acc: 0.9854\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Fold 4 Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.8947    0.8750    0.8848       272\n           1     0.9553    0.9629    0.9591       755\n\n    accuracy                         0.9396      1027\n   macro avg     0.9250    0.9190    0.9219      1027\nweighted avg     0.9393    0.9396    0.9394      1027\n\nConfusion Matrix:\n[[238  34]\n [ 28 727]]\nFold 5/5\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3 - Loss: 0.2627 - Train Acc: 0.8891\nEpoch 2/3 - Loss: 0.0828 - Train Acc: 0.9711\nEpoch 3/3 - Loss: 0.0460 - Train Acc: 0.9842\nFold 5 Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.8139    0.9485    0.8761       272\n           1     0.9803    0.9219    0.9502       755\n\n    accuracy                         0.9289      1027\n   macro avg     0.8971    0.9352    0.9131      1027\nweighted avg     0.9362    0.9289    0.9305      1027\n\nConfusion Matrix:\n[[258  14]\n [ 59 696]]\n\n=== Overall Classification Report ===\nUBC-NLP/MARBERTv2\n\n=== Overall Classification Report ===\n              precision    recall  f1-score   support\n\n           0     0.8144    0.8802    0.8460      1361\n           1     0.9555    0.9277    0.9414      3777\n\n    accuracy                         0.9151      5138\n   macro avg     0.8850    0.9040    0.8937      5138\nweighted avg     0.9182    0.9151    0.9162      5138\n\n=== Overall Confusion Matrix ===\n[[1198  163]\n [ 273 3504]]\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"HEADS 4 0 stage HCMA +Vgta","metadata":{}},{"cell_type":"code","source":"\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models, transforms\nfrom transformers import AutoTokenizer, AutoModel\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nfrom torchvision.models import resnet50\nimport csv\nimport timm\n\n\nimport os, random, torch, numpy as np\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    ##torch.backends.cudnn.deterministic = False\n    ##torch.backends.cudnn.benchmark = True\n    torch.use_deterministic_algorithms(True, warn_only=False)\n    print(f\"[INFO] All seeds set to {seed}\")\n\nseed_everything(42)\n\ndef seed_worker(worker_id):\n    worker_seed = torch.initial_seed() % 2**32\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)\n\ng = torch.Generator()\ng.manual_seed(42)\n\ndef free_gpu_memory():\n    torch.cuda.empty_cache()\n    torch.cuda.ipc_collect()\n# --------------------- CONFIG ---------------------\n\nTEXT_MODEL_NAME = 'UBC-NLP/MARBERTv2'\nNUM_CLASSES = 2\nBATCH_SIZE = 8\nNUM_EPOCHS = 3\nFOLDS = 5\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndf = pd.read_csv(\"/kaggle/input/ekafnewsforkhawla/shuffled_cleaned_text_file for khawla.txt\", sep=\"\\t\",encoding=\"utf-8\")\nprint(len(df))\n\nimage_encoder = models.efficientnet_b1(weights=models.EfficientNet_B1_Weights.DEFAULT)\n\nimage_encoder.classifier = nn.Identity()\n\n\nclass CrossAttentionLayer(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n        self.norm = nn.LayerNorm(embed_dim)\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, embed_dim),\n            nn.ReLU(),\n            nn.Linear(embed_dim, embed_dim)\n        )\n        self.dropout = nn.Dropout(0.4)\n\n    def forward(self, query, key, value, key_padding_mask=None):\n        attn_output, _ = self.attn(query, key, value, key_padding_mask=key_padding_mask)\n        x = self.norm(query + self.dropout(attn_output))\n        ff_output = self.norm(x + self.dropout(self.ff(x)))\n        return ff_output\n\n# Define the Full Multimodal Classifier\nclass Multimodal_Arabic_Fake_news_Identification_via_Hybrid_Attention_Networks(nn.Module):\n    def __init__(self, text_model_name, image_encoder, num_classes):\n        super().__init__()\n\n        # Text encoder (e.g., BERT)\n        self.text_encoder = AutoModel.from_pretrained(text_model_name)\n        self.text_proj = nn.Linear(self.text_encoder.config.hidden_size, 512)\n\n        # Image encoder (e.g., ResNet, ViT)\n        self.image_encoder = image_encoder \n        self.image_proj = nn.Linear(1280, 512)\n\n        # Cross-modal interaction attention\n        self.cross_modal_attn = CrossAttentionLayer(embed_dim=512, num_heads=4)\n\n        # Final classifier\n        self.classifier = nn.Sequential(nn.Linear(512, 256),nn.ReLU(),nn.Linear(256, num_classes))\n    def forward(self, input_ids, attention_mask, image):\n        # Encode text\n        text_output = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n        text_embeds = self.text_proj(text_output.last_hidden_state) \n\n        # Encode image\n        image_feats1 = self.image_encoder(image) \n        image_feats = self.image_proj(image_feats1).unsqueeze(1) \n        visual_guided_text = self.cross_modal_attn(query=image_feats, key=text_embeds, value=text_embeds)\n        \n        final_multi_repr = torch.mean(visual_guided_text, dim=1) \n\n        output = self.classifier(final_multi_repr)\n        return output\n\n\n# --------------------- DATASET ---------------------\nclass ArabicMultimodal_Fake_News_Dataset(Dataset):\n    def __init__(self, samples, tokenizer, transform):\n        self.samples = samples\n        self.tokenizer = tokenizer\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        sample = self.samples[idx]\n        ##image = Image.open(sample['image_path']).convert('RGB')\n        image = Image.open(sample[1]).convert('RGB')\n        image = self.transform(image)\n\n        text_enc = self.tokenizer(\n            sample[0], padding='max_length', truncation=True, max_length=64, return_tensors='pt'\n        )\n        return {\n            'input_ids': text_enc['input_ids'].squeeze(0),\n            'attention_mask': text_enc['attention_mask'].squeeze(0),\n            'image': image,\n            'label': torch.tensor(sample[2], dtype=torch.long)\n        }\n\n# --------------------- UTILS ---------------------\n\n\ndef load_data(annotation_file, image_root):\n    samples = []\n    label2folder = {0: 'Fake', 1: 'Real'}\n\n    with open(annotation_file, 'r', encoding='utf-8') as f:\n        reader = csv.DictReader(f, delimiter='\\t')\n        for row in reader:\n            tweet_id = row['id']\n            text = row['preprocess1']\n            label = int(row['label'])\n            labelstr=row['label']\n            dataset = row['dataset']\n\n            folder_name = label2folder[label]\n\n            # Try different extensions\n            image_path = None\n            for ext in ['jpg', 'png', 'jpeg']:\n                temp_path = os.path.join(image_root, folder_name, f\"{tweet_id}.{ext}\")\n                if os.path.exists(temp_path):\n                    image_path = temp_path\n                    break\n\n            if image_path:\n                samples.append((text, image_path, label,dataset))\n            else:\n                print(f\"Image not found for {tweet_id} with label {label}\")\n\n    return samples\n\n# --------------------- TRAIN + EVAL ---------------------\ndef train_model(model, dataloader, optimizer, criterion):\n    model.train()\n    total_loss, total_correct = 0, 0\n    for batch in dataloader:\n        input_ids = batch['input_ids'].to(DEVICE)\n        attention_mask = batch['attention_mask'].to(DEVICE)\n        images = batch['image'].to(DEVICE)\n        labels = batch['label'].to(DEVICE)\n\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask, images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        preds = outputs.argmax(dim=1)\n        total_correct += (preds == labels).sum().item()\n    return total_loss / len(dataloader), total_correct / len(dataloader.dataset)\n\ndef evaluate_model(model, dataloader):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids = batch['input_ids'].to(DEVICE)\n            attention_mask = batch['attention_mask'].to(DEVICE)\n            images = batch['image'].to(DEVICE)\n            labels = batch['label'].to(DEVICE)\n\n            outputs = model(input_ids, attention_mask, images)\n            preds = outputs.argmax(dim=1)\n\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    return all_preds, all_labels\n\n\n# --------------------- MAIN ---------------------\nif __name__ == '__main__':\n    ##label2idx = {'Fake': 1, 'Real': 0}\n    label2idx = {1: 1, 0: 0}  \n    idx2label = {v: k for k, v in label2idx.items()}\n    samples = load_data(\n    '/kaggle/input/ekafnewsforkhawla/shuffled_cleaned_text_file for khawla.txt',\n    '/kaggle/input/ekafnewsforkhawla/sorted_imagesOur')\n    \n    tokenizer = AutoTokenizer.from_pretrained(TEXT_MODEL_NAME)\n    transform = models.EfficientNet_B1_Weights.DEFAULT.transforms()\n    \n    labels = [sample[2] for sample in samples]\n    \n    strat =labels\n    skf = StratifiedKFold(n_splits=FOLDS, shuffle=False, random_state=None)\n    all_preds, all_trues = [], []\n    for fold, (train_idx, val_idx) in enumerate(skf.split(samples, strat)):\n        print(f\"Fold {fold + 1}/{FOLDS}\")\n        train_samples = [samples[i] for i in train_idx]\n        val_samples = [samples[i] for i in val_idx]\n\n        train_dataset = ArabicMultimodal_Fake_News_Dataset(train_samples, tokenizer, transform)\n        val_dataset = ArabicMultimodal_Fake_News_Dataset(val_samples, tokenizer, transform)\n\n        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False,worker_init_fn=seed_worker, generator=g)\n        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,worker_init_fn=seed_worker, generator=g)\n\n        model = Multimodal_Arabic_Fake_news_Identification_via_Hybrid_Attention_Networks(text_model_name=TEXT_MODEL_NAME,image_encoder=image_encoder,num_classes=NUM_CLASSES).to(DEVICE)\n        optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n        criterion = nn.CrossEntropyLoss()\n\n        for epoch in range(NUM_EPOCHS):\n            train_loss, train_acc = train_model(model, train_loader, optimizer, criterion)\n            print(f\"Epoch {epoch + 1}/{NUM_EPOCHS} - Loss: {train_loss:.4f} - Train Acc: {train_acc:.4f}\")\n\n        preds, trues = evaluate_model(model, val_loader)\n        print(f\"Fold {fold + 1} Classification Report:\")\n        print(classification_report(trues, preds, target_names=[\"0\",\"1\"], digits=4))\n        print(\"Confusion Matrix:\")\n        print(confusion_matrix(trues, preds))\n\n        all_preds.extend(preds)\n        all_trues.extend(trues)\n        free_gpu_memory()\n\n    print(\"\\n=== Overall Classification Report ===\")\n    print(TEXT_MODEL_NAME)\n    print(\"\\n=== Overall Classification Report ===\")\n    print(classification_report(all_trues, all_preds, target_names=[\"0\",\"1\"], digits=4))\n    print(\"=== Overall Confusion Matrix ===\")\n    print(confusion_matrix(all_trues, all_preds))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T22:03:33.052421Z","iopub.execute_input":"2026-01-31T22:03:33.052659Z","iopub.status.idle":"2026-01-31T22:37:26.679786Z","shell.execute_reply.started":"2026-01-31T22:03:33.052636Z","shell.execute_reply":"2026-01-31T22:37:26.678962Z"}},"outputs":[{"name":"stdout","text":"[INFO] All seeds set to 42\n5138\nFold 1/5\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3 - Loss: 0.4596 - Train Acc: 0.7835\nEpoch 2/3 - Loss: 0.2259 - Train Acc: 0.9051\nEpoch 3/3 - Loss: 0.0895 - Train Acc: 0.9684\nFold 1 Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.7639    0.8566    0.8076       272\n           1     0.9461    0.9048    0.9249       756\n\n    accuracy                         0.8920      1028\n   macro avg     0.8550    0.8807    0.8663      1028\nweighted avg     0.8979    0.8920    0.8939      1028\n\nConfusion Matrix:\n[[233  39]\n [ 72 684]]\nFold 2/5\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3 - Loss: 0.4033 - Train Acc: 0.8073\nEpoch 2/3 - Loss: 0.1706 - Train Acc: 0.9316\nEpoch 3/3 - Loss: 0.0623 - Train Acc: 0.9803\nFold 2 Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.7895    0.8824    0.8333       272\n           1     0.9558    0.9153    0.9351       756\n\n    accuracy                         0.9066      1028\n   macro avg     0.8726    0.8988    0.8842      1028\nweighted avg     0.9118    0.9066    0.9082      1028\n\nConfusion Matrix:\n[[240  32]\n [ 64 692]]\nFold 3/5\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3 - Loss: 0.3799 - Train Acc: 0.8316\nEpoch 2/3 - Loss: 0.1360 - Train Acc: 0.9496\nEpoch 3/3 - Loss: 0.0624 - Train Acc: 0.9808\nFold 3 Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.7914    0.8755    0.8313       273\n           1     0.9532    0.9166    0.9345       755\n\n    accuracy                         0.9056      1028\n   macro avg     0.8723    0.8960    0.8829      1028\nweighted avg     0.9102    0.9056    0.9071      1028\n\nConfusion Matrix:\n[[239  34]\n [ 63 692]]\nFold 4/5\nEpoch 1/3 - Loss: 0.3363 - Train Acc: 0.8523\nEpoch 2/3 - Loss: 0.1045 - Train Acc: 0.9613\nEpoch 3/3 - Loss: 0.0430 - Train Acc: 0.9852\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Fold 4 Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.7823    0.9118    0.8421       272\n           1     0.9662    0.9086    0.9365       755\n\n    accuracy                         0.9094      1027\n   macro avg     0.8743    0.9102    0.8893      1027\nweighted avg     0.9175    0.9094    0.9115      1027\n\nConfusion Matrix:\n[[248  24]\n [ 69 686]]\nFold 5/5\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3 - Loss: 0.2897 - Train Acc: 0.8762\nEpoch 2/3 - Loss: 0.0867 - Train Acc: 0.9715\nEpoch 3/3 - Loss: 0.0529 - Train Acc: 0.9849\nFold 5 Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.8781    0.9007    0.8893       272\n           1     0.9639    0.9550    0.9594       755\n\n    accuracy                         0.9406      1027\n   macro avg     0.9210    0.9279    0.9244      1027\nweighted avg     0.9412    0.9406    0.9408      1027\n\nConfusion Matrix:\n[[245  27]\n [ 34 721]]\n\n=== Overall Classification Report ===\nUBC-NLP/MARBERTv2\n\n=== Overall Classification Report ===\n              precision    recall  f1-score   support\n\n           0     0.7996    0.8854    0.8403      1361\n           1     0.9570    0.9200    0.9382      3777\n\n    accuracy                         0.9109      5138\n   macro avg     0.8783    0.9027    0.8892      5138\nweighted avg     0.9153    0.9109    0.9123      5138\n\n=== Overall Confusion Matrix ===\n[[1205  156]\n [ 302 3475]]\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"HEADS 4 4 stage Vgta HCMA Marbert Effb1","metadata":{}},{"cell_type":"code","source":"\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models, transforms\nfrom transformers import AutoTokenizer, AutoModel\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nfrom torchvision.models import resnet50\nimport csv\nimport timm\n\n\nimport os, random, torch, numpy as np\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    ##torch.backends.cudnn.deterministic = False\n    ##torch.backends.cudnn.benchmark = True\n    torch.use_deterministic_algorithms(True, warn_only=False)\n    print(f\"[INFO] All seeds set to {seed}\")\n\nseed_everything(42)\n\ndef seed_worker(worker_id):\n    worker_seed = torch.initial_seed() % 2**32\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)\n\ng = torch.Generator()\ng.manual_seed(42)\n\ndef free_gpu_memory():\n    torch.cuda.empty_cache()\n    torch.cuda.ipc_collect()\n# --------------------- CONFIG ---------------------\n\nTEXT_MODEL_NAME = 'UBC-NLP/MARBERTv2'\nNUM_CLASSES = 2\nBATCH_SIZE = 8\nNUM_EPOCHS = 3\nFOLDS = 5\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndf = pd.read_csv(\"/kaggle/input/ekafnewsforkhawla/shuffled_cleaned_text_file for khawla.txt\", sep=\"\\t\",encoding=\"utf-8\")\nprint(len(df))\n\nimage_encoder = models.efficientnet_b1(weights=models.EfficientNet_B1_Weights.DEFAULT)\n\nimage_encoder.classifier = nn.Identity()\n\n\n\n# Define Cross-Attention Block\n\nclass CrossAttentionLayer(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n        self.norm = nn.LayerNorm(embed_dim)\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, embed_dim),\n            nn.ReLU(),\n            nn.Linear(embed_dim, embed_dim)\n        )\n        self.dropout = nn.Dropout(0.4)\n\n    def forward(self, query, key, value, key_padding_mask=None):\n        attn_output, _ = self.attn(query, key, value, key_padding_mask=key_padding_mask)\n        x = self.norm(query + self.dropout(attn_output))\n        ff_output = self.norm(x + self.dropout(self.ff(x)))\n        return ff_output\n\n# Define the Full Multimodal Classifier\nclass Multimodal_Arabic_Fake_news_Identification_via_Hybrid_Attention_Networks(nn.Module):\n    def __init__(self, text_model_name, image_encoder, num_classes):\n        super().__init__()\n        self.text_encoder = AutoModel.from_pretrained(text_model_name)\n        self.text_proj = nn.Linear(self.text_encoder.config.hidden_size, 512)\n        self.image_encoder = image_encoder\n        self.image_proj = nn.Linear(1280, 512)\n        self.cross_modal_attn = CrossAttentionLayer(embed_dim=512, num_heads=4)\n        self.classifier = nn.Sequential(nn.Linear(512, 256),nn.ReLU(),nn.Linear(256, num_classes))\n    def forward(self, input_ids, attention_mask, image):\n        # Encode text\n        text_output = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n        text_embeds = self.text_proj(text_output.last_hidden_state)  # (B, L, 512)\n\n        # Encode image\n        image_feats1 = self.image_encoder(image) \n        image_feats = self.image_proj(image_feats1).unsqueeze(1)  # (B, 1, 512)\n        # Cross-modal attention - level 1 \n        img_cross = self.cross_modal_attn(query=text_embeds, key=image_feats, value=image_feats)\n        txt_cross = self.cross_modal_attn(query=image_feats, key=text_embeds, value=text_embeds,key_padding_mask=~attention_mask.bool())\n        \n        # Cross-modal attention - level 2 \n        cross_modal_img2txt = self.cross_modal_attn(query=txt_cross, key=img_cross, value=img_cross)\n        cross_modal_txt2img = self.cross_modal_attn(query=img_cross, key=txt_cross, value=txt_cross)\n        \n        # Cross-modal attention - level 3 \n        cross_modal_img2txt1 = self.cross_modal_attn(query=cross_modal_txt2img, key=cross_modal_img2txt, value=cross_modal_img2txt)\n        cross_modal_txt2img1 = self.cross_modal_attn(query=cross_modal_img2txt, key=cross_modal_txt2img, value=cross_modal_txt2img)\n        # Cross-modal attention - level 4   \n        cross_modal_img2txt2 = self.cross_modal_attn(query=cross_modal_txt2img1, key=cross_modal_img2txt1, value=cross_modal_img2txt1)\n        cross_modal_txt2img2 = self.cross_modal_attn(query=cross_modal_img2txt1, key=cross_modal_txt2img1, value=cross_modal_txt2img1)\n\n        # Visual Guided Text attention\n        visual_guided_text = self.cross_modal_attn(query=cross_modal_img2txt2, key=cross_modal_txt2img2, value=cross_modal_txt2img2)\n\n        # Mean pooling\n        final_multi_repr = torch.mean(visual_guided_text, dim=1)  # (B, 512)\n\n        output = self.classifier(final_multi_repr)\n        return output\n\n\n# --------------------- DATASET ---------------------\nclass ArabicMultimodal_Fake_News_Dataset(Dataset):\n    def __init__(self, samples, tokenizer, transform):\n        self.samples = samples\n        self.tokenizer = tokenizer\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        sample = self.samples[idx]\n        ##image = Image.open(sample['image_path']).convert('RGB')\n        image = Image.open(sample[1]).convert('RGB')\n        image = self.transform(image)\n\n        text_enc = self.tokenizer(\n            sample[0], padding='max_length', truncation=True, max_length=64, return_tensors='pt'\n        )\n        return {\n            'input_ids': text_enc['input_ids'].squeeze(0),\n            'attention_mask': text_enc['attention_mask'].squeeze(0),\n            'image': image,\n            'label': torch.tensor(sample[2], dtype=torch.long)\n        }\n\n# --------------------- UTILS ---------------------\n\n\ndef load_data(annotation_file, image_root):\n    samples = []\n    label2folder = {0: 'Fake', 1: 'Real'}\n\n    with open(annotation_file, 'r', encoding='utf-8') as f:\n        reader = csv.DictReader(f, delimiter='\\t')\n        for row in reader:\n            tweet_id = row['id']\n            text = row['preprocess1']\n            label = int(row['label'])\n            labelstr=row['label']\n            dataset = row['dataset']\n\n            folder_name = label2folder[label]\n\n            # Try different extensions\n            image_path = None\n            for ext in ['jpg', 'png', 'jpeg']:\n                temp_path = os.path.join(image_root, folder_name, f\"{tweet_id}.{ext}\")\n                if os.path.exists(temp_path):\n                    image_path = temp_path\n                    break\n\n            if image_path:\n                samples.append((text, image_path, label,dataset))\n            else:\n                print(f\"Image not found for {tweet_id} with label {label}\")\n\n    return samples\n\n# --------------------- TRAIN + EVAL ---------------------\ndef train_model(model, dataloader, optimizer, criterion):\n    model.train()\n    total_loss, total_correct = 0, 0\n    for batch in dataloader:\n        input_ids = batch['input_ids'].to(DEVICE)\n        attention_mask = batch['attention_mask'].to(DEVICE)\n        images = batch['image'].to(DEVICE)\n        labels = batch['label'].to(DEVICE)\n\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask, images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        preds = outputs.argmax(dim=1)\n        total_correct += (preds == labels).sum().item()\n    return total_loss / len(dataloader), total_correct / len(dataloader.dataset)\n\ndef evaluate_model(model, dataloader):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids = batch['input_ids'].to(DEVICE)\n            attention_mask = batch['attention_mask'].to(DEVICE)\n            images = batch['image'].to(DEVICE)\n            labels = batch['label'].to(DEVICE)\n\n            outputs = model(input_ids, attention_mask, images)\n            preds = outputs.argmax(dim=1)\n\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    return all_preds, all_labels\n\n\n# --------------------- MAIN ---------------------\nif __name__ == '__main__':\n    ##label2idx = {'Fake': 1, 'Real': 0}\n    label2idx = {1: 1, 0: 0}  \n    idx2label = {v: k for k, v in label2idx.items()}\n    samples = load_data(\n    '/kaggle/input/ekafnewsforkhawla/shuffled_cleaned_text_file for khawla.txt',\n    '/kaggle/input/ekafnewsforkhawla/sorted_imagesOur')\n    \n    tokenizer = AutoTokenizer.from_pretrained(TEXT_MODEL_NAME)\n    transform = models.EfficientNet_B1_Weights.DEFAULT.transforms()\n    \n    labels = [sample[2] for sample in samples]\n    \n    strat =labels\n    skf = StratifiedKFold(n_splits=FOLDS, shuffle=False, random_state=None)\n    all_preds, all_trues = [], []\n    for fold, (train_idx, val_idx) in enumerate(skf.split(samples, strat)):\n        print(f\"Fold {fold + 1}/{FOLDS}\")\n        train_samples = [samples[i] for i in train_idx]\n        val_samples = [samples[i] for i in val_idx]\n\n        train_dataset = ArabicMultimodal_Fake_News_Dataset(train_samples, tokenizer, transform)\n        val_dataset = ArabicMultimodal_Fake_News_Dataset(val_samples, tokenizer, transform)\n\n        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False,worker_init_fn=seed_worker, generator=g)\n        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,worker_init_fn=seed_worker, generator=g)\n\n        model = Multimodal_Arabic_Fake_news_Identification_via_Hybrid_Attention_Networks(text_model_name=TEXT_MODEL_NAME,image_encoder=image_encoder,num_classes=NUM_CLASSES).to(DEVICE)\n        optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n        criterion = nn.CrossEntropyLoss()\n\n        for epoch in range(NUM_EPOCHS):\n            train_loss, train_acc = train_model(model, train_loader, optimizer, criterion)\n            print(f\"Epoch {epoch + 1}/{NUM_EPOCHS} - Loss: {train_loss:.4f} - Train Acc: {train_acc:.4f}\")\n\n        preds, trues = evaluate_model(model, val_loader)\n        print(f\"Fold {fold + 1} Classification Report:\")\n        print(classification_report(trues, preds, target_names=[\"0\",\"1\"], digits=4))\n        print(\"Confusion Matrix:\")\n        print(confusion_matrix(trues, preds))\n\n        all_preds.extend(preds)\n        all_trues.extend(trues)\n        free_gpu_memory()\n\n    print(\"\\n=== Overall Classification Report ===\")\n    print(TEXT_MODEL_NAME)\n    print(\"\\n=== Overall Classification Report ===\")\n    print(classification_report(all_trues, all_preds, target_names=[\"0\",\"1\"], digits=4))\n    print(\"=== Overall Confusion Matrix ===\")\n    print(confusion_matrix(all_trues, all_preds))\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T22:37:26.681085Z","iopub.execute_input":"2026-01-31T22:37:26.681451Z","iopub.status.idle":"2026-01-31T23:14:47.272226Z","shell.execute_reply.started":"2026-01-31T22:37:26.681425Z","shell.execute_reply":"2026-01-31T23:14:47.271424Z"}},"outputs":[{"name":"stdout","text":"[INFO] All seeds set to 42\n5138\nFold 1/5\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3 - Loss: 0.4730 - Train Acc: 0.7676\nEpoch 2/3 - Loss: 0.2324 - Train Acc: 0.9058\nEpoch 3/3 - Loss: 0.1055 - Train Acc: 0.9623\nFold 1 Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.8261    0.8382    0.8321       272\n           1     0.9415    0.9365    0.9390       756\n\n    accuracy                         0.9105      1028\n   macro avg     0.8838    0.8874    0.8856      1028\nweighted avg     0.9110    0.9105    0.9107      1028\n\nConfusion Matrix:\n[[228  44]\n [ 48 708]]\nFold 2/5\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3 - Loss: 0.3907 - Train Acc: 0.8260\nEpoch 2/3 - Loss: 0.1531 - Train Acc: 0.9418\nEpoch 3/3 - Loss: 0.0747 - Train Acc: 0.9759\nFold 2 Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.8077    0.8493    0.8280       272\n           1     0.9447    0.9272    0.9359       756\n\n    accuracy                         0.9066      1028\n   macro avg     0.8762    0.8883    0.8819      1028\nweighted avg     0.9085    0.9066    0.9073      1028\n\nConfusion Matrix:\n[[231  41]\n [ 55 701]]\nFold 3/5\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3 - Loss: 0.3575 - Train Acc: 0.8399\nEpoch 2/3 - Loss: 0.1476 - Train Acc: 0.9438\nEpoch 3/3 - Loss: 0.0700 - Train Acc: 0.9788\nFold 3 Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.8775    0.8132    0.8441       273\n           1     0.9342    0.9589    0.9464       755\n\n    accuracy                         0.9202      1028\n   macro avg     0.9058    0.8861    0.8953      1028\nweighted avg     0.9191    0.9202    0.9192      1028\n\nConfusion Matrix:\n[[222  51]\n [ 31 724]]\nFold 4/5\nEpoch 1/3 - Loss: 0.3225 - Train Acc: 0.8638\nEpoch 2/3 - Loss: 0.1040 - Train Acc: 0.9664\nEpoch 3/3 - Loss: 0.0638 - Train Acc: 0.9801\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Fold 4 Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9370    0.8750    0.9049       272\n           1     0.9560    0.9788    0.9673       755\n\n    accuracy                         0.9513      1027\n   macro avg     0.9465    0.9269    0.9361      1027\nweighted avg     0.9510    0.9513    0.9508      1027\n\nConfusion Matrix:\n[[238  34]\n [ 16 739]]\nFold 5/5\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3 - Loss: 0.2753 - Train Acc: 0.8883\nEpoch 2/3 - Loss: 0.0920 - Train Acc: 0.9698\nEpoch 3/3 - Loss: 0.0496 - Train Acc: 0.9822\nFold 5 Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.8312    0.9412    0.8828       272\n           1     0.9777    0.9311    0.9539       755\n\n    accuracy                         0.9338      1027\n   macro avg     0.9045    0.9362    0.9183      1027\nweighted avg     0.9389    0.9338    0.9350      1027\n\nConfusion Matrix:\n[[256  16]\n [ 52 703]]\n\n=== Overall Classification Report ===\nUBC-NLP/MARBERTv2\n\n=== Overall Classification Report ===\n              precision    recall  f1-score   support\n\n           0     0.8533    0.8633    0.8583      1361\n           1     0.9505    0.9465    0.9485      3777\n\n    accuracy                         0.9245      5138\n   macro avg     0.9019    0.9049    0.9034      5138\nweighted avg     0.9248    0.9245    0.9246      5138\n\n=== Overall Confusion Matrix ===\n[[1175  186]\n [ 202 3575]]\n","output_type":"stream"}],"execution_count":5}]}